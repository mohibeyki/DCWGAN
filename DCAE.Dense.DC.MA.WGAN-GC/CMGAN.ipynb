{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CMGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python_defaultSpec_1600481393982",
      "display_name": "Python 3.8.5 64-bit"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZKq_k35W6_a"
      },
      "source": [
        "import argparse\n",
        "import copy\n",
        "import gc\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import h5py\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or158iZXcTtF",
        "tags": [],
        "outputId": "157afc77-a51a-42da-8159-efedfcfc52db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "experimentName = 'DCW-GAN'\n",
        "\n",
        "parser.add_argument(\"--DATASETPATH\", type=str, default=os.path.expanduser('~/workspace/data/mimic-iii-processed/BINARY.h5'), help=\"Dataset file\")\n",
        "\n",
        "parser.add_argument(\"--n_epochs\", type=int, default=100, help=\"number of epochs of training\")\n",
        "parser.add_argument(\"--n_epochs_ae\", type=int, default=100, help=\"number of epochs of pretraining the autoencoder\")\n",
        "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n",
        "parser.add_argument(\"--lr\", type=float, default=0.001, help=\"adam: learning rate\")\n",
        "parser.add_argument(\"--weight_decay\", type=float, default=0.0001, help=\"l2 regularization\")\n",
        "parser.add_argument(\"--b1\", type=float, default=0.9, help=\"adam: decay of first order momentum of gradient\")\n",
        "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
        "parser.add_argument(\"--n_cpu\", type=int, default=32, help=\"number of cpu threads to use during batch generation\")\n",
        "parser.add_argument('--n_critic', type=int, default=5, help='number of D iters per each G iter')\n",
        "parser.add_argument('--clamp', type=float, default=0.01)\n",
        "\n",
        "parser.add_argument(\"--cuda\", type=bool, default=True, help=\"CUDA activation\")\n",
        "parser.add_argument(\"--multiplegpu\", type=bool, default=True, help=\"number of cpu threads to use during batch generation\")\n",
        "parser.add_argument(\"--num_gpu\", type=int, default=1, help=\"Number of GPUs in case of multiple GPU\")\n",
        "\n",
        "parser.add_argument(\"--latent_dim\", type=int, default=128, help=\"dimensionality of the latent space\")\n",
        "parser.add_argument(\"--sample_interval\", type=int, default=100, help=\"interval between samples\")\n",
        "parser.add_argument(\"--epoch_time_show\", type=bool, default=True, help=\"interval betwen image samples\")\n",
        "parser.add_argument(\"--epoch_save_model_freq\", type=int, default=100, help=\"number of epops per model save\")\n",
        "parser.add_argument(\"--minibatch_averaging\", type=bool, default=False, help=\"Minibatch averaging\")\n",
        "\n",
        "parser.add_argument(\"--expPATH\", type=str, default=os.path.expanduser('~/workspace/experiments/pytorch/model/{}'.format(experimentName)), help=\"Training status\")\n",
        "\n",
        "opt = parser.parse_args([])\n",
        "print(opt)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(DATASETPATH='/home/mohi/workspace/data/mimic-iii-processed/BINARY.h5', b1=0.9, b2=0.999, batch_size=64, clamp=0.01, cuda=True, epoch_save_model_freq=100, epoch_time_show=True, expPATH='/home/mohi/workspace/experiments/pytorch/model/DCW-GAN', latent_dim=128, lr=0.001, minibatch_averaging=False, multiplegpu=True, n_cpu=32, n_critic=5, n_epochs=100, n_epochs_ae=100, num_gpu=1, sample_interval=100, weight_decay=0.0001)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3syZZGE3ixcZ",
        "tags": [],
        "outputId": "7585c8ef-bf5e-4fbe-b6a4-10cf2582964c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "######################\n",
        "### Initialization ###\n",
        "######################\n",
        "\n",
        "# Create experiments DIR\n",
        "if not os.path.exists(opt.expPATH):\n",
        "    os.system('mkdir {0}'.format(opt.expPATH))\n",
        "\n",
        "# opt.seed = 1024 # fix seed\n",
        "opt.seed = random.randint(1, 10000)\n",
        "\n",
        "print('Random Seed: {}'.format(opt.seed))\n",
        "random.seed(opt.seed)\n",
        "torch.manual_seed(opt.seed)\n",
        "np.random.seed(opt.seed)\n",
        "cudnn.benchmark = True\n",
        "\n",
        "if torch.cuda.is_available() and not opt.cuda:\n",
        "    print(\"WARNING: You have a CUDA device BUT it is not in use...\")\n",
        "\n",
        "device = torch.device(\"cuda:0\" if opt.cuda else \"cpu\")\n",
        "print('using \\'{}\\' as the tensor processor'.format(device))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed: 6232\n",
            "using 'cuda:0' as the tensor processor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYJtIeTEil-P"
      },
      "source": [
        "#################################\n",
        "### Reading Dataset from File ###\n",
        "#################################\n",
        "\n",
        "input_data = None\n",
        "with h5py.File(opt.DATASETPATH, 'r') as hf:\n",
        "    input_data = hf.get('dataset')[()]\n",
        "\n",
        "total_samples = input_data.shape[0]\n",
        "feature_size = input_data.shape[1]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP96S8Kbcs0y"
      },
      "source": [
        "#####################\n",
        "### Dataset Model ###\n",
        "#####################\n",
        "\n",
        "class EHRDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "        self.sample_size = dataset.shape[0]\n",
        "        self.feature_size = dataset.shape[1]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dataset[idx]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXkAxe6HS-KJ",
        "tags": [],
        "outputId": "b0fb544b-0305-4a1b-e2a8-589e737e572d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "##########################\n",
        "### Dataset Processing ###\n",
        "##########################\n",
        "\n",
        "train_data = input_data[:int(0.8 * total_samples)]\n",
        "test_data = input_data[int(0.8 * total_samples):]\n",
        "print('total samples: {}, features: {}'.format(total_samples, feature_size))\n",
        "print('training data shape: {}, testing data shape: {}, dataset type: {}'.format(train_data.shape, test_data.shape, input_data.dtype))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total samples: 46520, features: 1071\n",
            "training data shape: (37216, 1071), testing data shape: (9304, 1071), dataset type: float32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvS8xM1k8Teu"
      },
      "source": [
        "training_dataloader = DataLoader(\n",
        "    EHRDataset(dataset=train_data),\n",
        "    batch_size=opt.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=opt.n_cpu\n",
        ")\n",
        "\n",
        "testing_dataloader = DataLoader(\n",
        "    EHRDataset(dataset=test_data),\n",
        "    batch_size=opt.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=opt.n_cpu\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcvJBicPcyhv"
      },
      "source": [
        "def weightsInit(m):\n",
        "    \"\"\"\n",
        "    Custom weight initialization.\n",
        "    :param m: Input argument to extract layer type\n",
        "    :return: Initialized architecture\n",
        "    \"\"\"\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jSYLExToytb"
      },
      "source": [
        "########################\n",
        "### AutoEncoder Loss ###\n",
        "########################\n",
        "\n",
        "class AutoEncoderLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AutoEncoderLoss, self).__init__()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        epsilon = 1e-12\n",
        "        term = target * torch.log(input + epsilon) + (1. - target) * torch.log(1. - input + epsilon)\n",
        "        return torch.mean(-torch.sum(term, 1), 0)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch3VtsBwn5p9"
      },
      "source": [
        "#########################\n",
        "### AutoEncoder Model ###\n",
        "#########################\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=5, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv1d(in_channels=8, out_channels=16, kernel_size=5, stride=2, padding=1),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, stride=3, padding=1),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=8, stride=3, padding=0),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose1d(in_channels=64, out_channels=32, kernel_size=8, stride=3, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose1d(in_channels=32, out_channels=16, kernel_size=5, stride=3, padding=1),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose1d(in_channels=16, out_channels=8, kernel_size=5, stride=2, padding=1),\n",
        "            nn.BatchNorm1d(8),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose1d(in_channels=8, out_channels=1, kernel_size=5, stride=2, padding=1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x.view(-1, 1, feature_size))\n",
        "\n",
        "    def decode(self, x):\n",
        "        return torch.squeeze(self.decoder(x.view(-1, 64, 28)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decode(self.encode(x))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgp1ytGXFDwS",
        "outputId": "83189359-2670-41a8-d812-4a497fdc1861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "############################\n",
        "### Model Initialization ###\n",
        "############################\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "autoencoder = Autoencoder()\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if opt.cuda else torch.FloatTensor\n",
        "\n",
        "if opt.cuda:\n",
        "    autoencoder.cuda()\n",
        "\n",
        "optimizer_A = torch.optim.Adam(autoencoder.parameters(), lr=opt.lr)\n",
        "autoencoder.apply(weightsInit)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Autoencoder(\n",
              "  (encoder): Sequential(\n",
              "    (0): Conv1d(1, 8, kernel_size=(5,), stride=(2,), padding=(1,))\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (2): Conv1d(8, 16, kernel_size=(5,), stride=(2,), padding=(1,))\n",
              "    (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (5): Conv1d(16, 32, kernel_size=(5,), stride=(3,), padding=(1,))\n",
              "    (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (8): Conv1d(32, 64, kernel_size=(8,), stride=(3,))\n",
              "    (9): Tanh()\n",
              "  )\n",
              "  (decoder): Sequential(\n",
              "    (0): ConvTranspose1d(64, 32, kernel_size=(8,), stride=(3,))\n",
              "    (1): ReLU()\n",
              "    (2): ConvTranspose1d(32, 16, kernel_size=(5,), stride=(3,), padding=(1,))\n",
              "    (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): ReLU()\n",
              "    (5): ConvTranspose1d(16, 8, kernel_size=(5,), stride=(2,), padding=(1,))\n",
              "    (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): ReLU()\n",
              "    (8): ConvTranspose1d(8, 1, kernel_size=(5,), stride=(2,), padding=(1,))\n",
              "    (9): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWlJt7_892vs",
        "tags": [],
        "outputId": "9683f9c2-ca2f-498a-ed76-52b66ebd57e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#####################################\n",
        "###### AutoEncoder Training #########\n",
        "#####################################\n",
        "\n",
        "criterion = AutoEncoderLoss()\n",
        "\n",
        "if True:\n",
        "    for epoch in range(opt.n_epochs_ae):\n",
        "\n",
        "        training_loss = 0\n",
        "        autoencoder.train()\n",
        "        for batch in training_dataloader:\n",
        "            batch = Variable(batch.type(Tensor))\n",
        "            generated = autoencoder(batch)\n",
        "\n",
        "            loss_A = criterion(generated, batch)\n",
        "\n",
        "            optimizer_A.zero_grad()\n",
        "            loss_A.backward()\n",
        "            optimizer_A.step()\n",
        "            training_loss += loss_A.item()\n",
        "\n",
        "        errors = 0\n",
        "        testing_loss = 0\n",
        "        autoencoder.eval()\n",
        "        for batch in testing_dataloader:\n",
        "            batch = Variable(batch.type(Tensor))\n",
        "            generated = autoencoder(batch)\n",
        "\n",
        "            res = generated.round()\n",
        "            diff = torch.abs(res - batch).view(1, 1, -1)[0][0].cpu().detach().numpy()\n",
        "            bad_diffs = diff[diff > 0.5]\n",
        "            errors += len(bad_diffs)\n",
        "            testing_loss += criterion(generated, batch)\n",
        "        print(\"[Epoch {:3d}/{:3d}] [Loss: {:10.2f}] [errors: {:6d}]\".format(epoch + 1, opt.n_epochs_ae, testing_loss, errors), flush=True)\n",
        "\n",
        "        print(\"[Epoch {:3d}/{:3d}] [Loss: {:10.2f}]\".format(epoch + 1, opt.n_epochs_ae, training_loss), flush=True)\n",
        "    torch.save(autoencoder.state_dict(), opt.expPATH + '/autoencoder.model')\n",
        "\n",
        "else:\n",
        "    autoencoder.load_state_dict(torch.load(opt.expPATH + '/autoencoder.model'))\n",
        "    autoencoder.eval()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch   1/100] [Loss:    6240.40] [errors: 131308]\n",
            "[Epoch   1/100] [Loss 88135.73]\n",
            "[Epoch   2/100] [Loss:    7997.56] [errors:  88159]\n",
            "[Epoch   2/100] [Loss 10170.56]\n",
            "[Epoch   3/100] [Loss:    1643.11] [errors:  69916]\n",
            "[Epoch   3/100] [Loss 5096.10]\n",
            "[Epoch   4/100] [Loss:    1326.59] [errors:  66208]\n",
            "[Epoch   4/100] [Loss 4020.32]\n",
            "[Epoch   5/100] [Loss:    1141.12] [errors:  65463]\n",
            "[Epoch   5/100] [Loss 3564.76]\n",
            "[Epoch   6/100] [Loss:    1065.53] [errors:  65258]\n",
            "[Epoch   6/100] [Loss 3318.05]\n",
            "[Epoch   7/100] [Loss:     988.93] [errors:  64550]\n",
            "[Epoch   7/100] [Loss 3121.21]\n",
            "[Epoch   8/100] [Loss:     919.84] [errors:  15725]\n",
            "[Epoch   8/100] [Loss 2918.24]\n",
            "[Epoch   9/100] [Loss:     833.59] [errors:  15231]\n",
            "[Epoch   9/100] [Loss 2681.43]\n",
            "[Epoch  10/100] [Loss:     731.47] [errors:  10679]\n",
            "[Epoch  10/100] [Loss 2360.36]\n",
            "[Epoch  11/100] [Loss:     573.78] [errors:   2121]\n",
            "[Epoch  11/100] [Loss 1957.94]\n",
            "[Epoch  12/100] [Loss:      42.48] [errors:    278]\n",
            "[Epoch  12/100] [Loss 407.42]\n",
            "[Epoch  13/100] [Loss:      12.60] [errors:     71]\n",
            "[Epoch  13/100] [Loss  65.98]\n",
            "[Epoch  14/100] [Loss:      16.35] [errors:    206]\n",
            "[Epoch  14/100] [Loss  33.46]\n",
            "[Epoch  15/100] [Loss:      10.80] [errors:    110]\n",
            "[Epoch  15/100] [Loss  19.15]\n",
            "[Epoch  16/100] [Loss:       8.82] [errors:     63]\n",
            "[Epoch  16/100] [Loss  19.78]\n",
            "[Epoch  17/100] [Loss:       3.58] [errors:     25]\n",
            "[Epoch  17/100] [Loss  15.90]\n",
            "[Epoch  18/100] [Loss:      11.38] [errors:    169]\n",
            "[Epoch  18/100] [Loss  18.54]\n",
            "[Epoch  19/100] [Loss:       5.50] [errors:     68]\n",
            "[Epoch  19/100] [Loss  10.26]\n",
            "[Epoch  20/100] [Loss:       3.67] [errors:     41]\n",
            "[Epoch  20/100] [Loss  10.46]\n",
            "[Epoch  21/100] [Loss:       2.61] [errors:     21]\n",
            "[Epoch  21/100] [Loss   5.93]\n",
            "[Epoch  22/100] [Loss:      20.57] [errors:    351]\n",
            "[Epoch  22/100] [Loss   3.60]\n",
            "[Epoch  23/100] [Loss:       4.10] [errors:     62]\n",
            "[Epoch  23/100] [Loss  20.68]\n",
            "[Epoch  24/100] [Loss:       3.36] [errors:     42]\n",
            "[Epoch  24/100] [Loss   7.26]\n",
            "[Epoch  25/100] [Loss:       5.03] [errors:     65]\n",
            "[Epoch  25/100] [Loss   4.37]\n",
            "[Epoch  26/100] [Loss:       4.60] [errors:     73]\n",
            "[Epoch  26/100] [Loss  11.93]\n",
            "[Epoch  27/100] [Loss:       7.05] [errors:    123]\n",
            "[Epoch  27/100] [Loss   7.53]\n",
            "[Epoch  28/100] [Loss:       2.30] [errors:     26]\n",
            "[Epoch  28/100] [Loss   4.25]\n",
            "[Epoch  29/100] [Loss:       4.18] [errors:     58]\n",
            "[Epoch  29/100] [Loss   6.59]\n",
            "[Epoch  30/100] [Loss:       2.59] [errors:     33]\n",
            "[Epoch  30/100] [Loss   3.66]\n",
            "[Epoch  31/100] [Loss:       5.82] [errors:     96]\n",
            "[Epoch  31/100] [Loss   4.14]\n",
            "[Epoch  32/100] [Loss:       2.36] [errors:     30]\n",
            "[Epoch  32/100] [Loss   8.23]\n",
            "[Epoch  33/100] [Loss:       2.73] [errors:     38]\n",
            "[Epoch  33/100] [Loss   4.07]\n",
            "[Epoch  34/100] [Loss:       4.55] [errors:     76]\n",
            "[Epoch  34/100] [Loss   5.86]\n",
            "[Epoch  35/100] [Loss:       2.45] [errors:     33]\n",
            "[Epoch  35/100] [Loss   6.71]\n",
            "[Epoch  36/100] [Loss:       2.89] [errors:     31]\n",
            "[Epoch  36/100] [Loss   2.73]\n",
            "[Epoch  37/100] [Loss:       2.50] [errors:     37]\n",
            "[Epoch  37/100] [Loss   3.79]\n",
            "[Epoch  38/100] [Loss:       2.56] [errors:     37]\n",
            "[Epoch  38/100] [Loss   4.04]\n",
            "[Epoch  39/100] [Loss:       2.48] [errors:     27]\n",
            "[Epoch  39/100] [Loss   5.55]\n",
            "[Epoch  40/100] [Loss:       2.07] [errors:     29]\n",
            "[Epoch  40/100] [Loss   2.36]\n",
            "[Epoch  41/100] [Loss:       5.09] [errors:     68]\n",
            "[Epoch  41/100] [Loss   3.60]\n",
            "[Epoch  42/100] [Loss:       2.66] [errors:     36]\n",
            "[Epoch  42/100] [Loss   3.76]\n",
            "[Epoch  43/100] [Loss:       3.04] [errors:     50]\n",
            "[Epoch  43/100] [Loss   3.03]\n",
            "[Epoch  44/100] [Loss:       3.18] [errors:     43]\n",
            "[Epoch  44/100] [Loss   4.89]\n",
            "[Epoch  45/100] [Loss:       1.72] [errors:     26]\n",
            "[Epoch  45/100] [Loss   1.53]\n",
            "[Epoch  46/100] [Loss:       2.86] [errors:     34]\n",
            "[Epoch  46/100] [Loss   3.22]\n",
            "[Epoch  47/100] [Loss:       2.88] [errors:     49]\n",
            "[Epoch  47/100] [Loss   5.25]\n",
            "[Epoch  48/100] [Loss:       2.23] [errors:     35]\n",
            "[Epoch  48/100] [Loss   3.09]\n",
            "[Epoch  49/100] [Loss:       1.75] [errors:     18]\n",
            "[Epoch  49/100] [Loss   2.53]\n",
            "[Epoch  50/100] [Loss:       1.28] [errors:     20]\n",
            "[Epoch  50/100] [Loss   0.92]\n",
            "[Epoch  51/100] [Loss:       1.57] [errors:     23]\n",
            "[Epoch  51/100] [Loss   0.80]\n",
            "[Epoch  52/100] [Loss:       1.93] [errors:     22]\n",
            "[Epoch  52/100] [Loss   0.54]\n",
            "[Epoch  53/100] [Loss:      15.39] [errors:    194]\n",
            "[Epoch  53/100] [Loss   4.97]\n",
            "[Epoch  54/100] [Loss:       1.80] [errors:     33]\n",
            "[Epoch  54/100] [Loss   5.89]\n",
            "[Epoch  55/100] [Loss:       2.12] [errors:     32]\n",
            "[Epoch  55/100] [Loss   0.57]\n",
            "[Epoch  56/100] [Loss:       5.86] [errors:     85]\n",
            "[Epoch  56/100] [Loss   2.32]\n",
            "[Epoch  57/100] [Loss:       1.60] [errors:     23]\n",
            "[Epoch  57/100] [Loss   3.34]\n",
            "[Epoch  58/100] [Loss:       2.11] [errors:     29]\n",
            "[Epoch  58/100] [Loss   2.82]\n",
            "[Epoch  59/100] [Loss:       1.91] [errors:     35]\n",
            "[Epoch  59/100] [Loss   2.46]\n",
            "[Epoch  60/100] [Loss:       1.18] [errors:     18]\n",
            "[Epoch  60/100] [Loss   3.02]\n",
            "[Epoch  61/100] [Loss:       1.26] [errors:     20]\n",
            "[Epoch  61/100] [Loss   1.21]\n",
            "[Epoch  62/100] [Loss:       2.48] [errors:     43]\n",
            "[Epoch  62/100] [Loss   0.92]\n",
            "[Epoch  63/100] [Loss:       3.45] [errors:     51]\n",
            "[Epoch  63/100] [Loss   4.20]\n",
            "[Epoch  64/100] [Loss:       2.51] [errors:     31]\n",
            "[Epoch  64/100] [Loss   2.79]\n",
            "[Epoch  65/100] [Loss:       1.86] [errors:     22]\n",
            "[Epoch  65/100] [Loss   0.84]\n",
            "[Epoch  66/100] [Loss:       1.69] [errors:     19]\n",
            "[Epoch  66/100] [Loss   0.37]\n",
            "[Epoch  67/100] [Loss:       1.73] [errors:     17]\n",
            "[Epoch  67/100] [Loss   0.20]\n",
            "[Epoch  68/100] [Loss:       2.05] [errors:     18]\n",
            "[Epoch  68/100] [Loss   0.18]\n",
            "[Epoch  69/100] [Loss:       2.98] [errors:     32]\n",
            "[Epoch  69/100] [Loss   0.71]\n",
            "[Epoch  70/100] [Loss:       1.32] [errors:     19]\n",
            "[Epoch  70/100] [Loss   8.34]\n",
            "[Epoch  71/100] [Loss:       1.16] [errors:     18]\n",
            "[Epoch  71/100] [Loss   0.72]\n",
            "[Epoch  72/100] [Loss:       1.34] [errors:     21]\n",
            "[Epoch  72/100] [Loss   0.54]\n",
            "[Epoch  73/100] [Loss:       2.60] [errors:     41]\n",
            "[Epoch  73/100] [Loss   1.74]\n",
            "[Epoch  74/100] [Loss:       3.01] [errors:     45]\n",
            "[Epoch  74/100] [Loss   3.19]\n",
            "[Epoch  75/100] [Loss:       3.24] [errors:     59]\n",
            "[Epoch  75/100] [Loss   2.44]\n",
            "[Epoch  76/100] [Loss:       1.64] [errors:     20]\n",
            "[Epoch  76/100] [Loss   1.36]\n",
            "[Epoch  77/100] [Loss:       1.57] [errors:     18]\n",
            "[Epoch  77/100] [Loss   0.21]\n",
            "[Epoch  78/100] [Loss:       1.53] [errors:     16]\n",
            "[Epoch  78/100] [Loss   0.13]\n",
            "[Epoch  79/100] [Loss:       1.48] [errors:     14]\n",
            "[Epoch  79/100] [Loss   0.12]\n",
            "[Epoch  80/100] [Loss:       5.10] [errors:     75]\n",
            "[Epoch  80/100] [Loss   2.88]\n",
            "[Epoch  81/100] [Loss:       1.08] [errors:     15]\n",
            "[Epoch  81/100] [Loss   2.77]\n",
            "[Epoch  82/100] [Loss:       2.03] [errors:     31]\n",
            "[Epoch  82/100] [Loss   0.94]\n",
            "[Epoch  83/100] [Loss:       2.04] [errors:     26]\n",
            "[Epoch  83/100] [Loss   2.87]\n",
            "[Epoch  84/100] [Loss:       2.50] [errors:     31]\n",
            "[Epoch  84/100] [Loss   1.78]\n",
            "[Epoch  85/100] [Loss:       1.80] [errors:     18]\n",
            "[Epoch  85/100] [Loss   1.38]\n",
            "[Epoch  86/100] [Loss:       1.46] [errors:     17]\n",
            "[Epoch  86/100] [Loss   0.84]\n",
            "[Epoch  87/100] [Loss:       1.59] [errors:     16]\n",
            "[Epoch  87/100] [Loss   0.56]\n",
            "[Epoch  88/100] [Loss:       1.52] [errors:     17]\n",
            "[Epoch  88/100] [Loss   0.53]\n",
            "[Epoch  89/100] [Loss:       3.80] [errors:     24]\n",
            "[Epoch  89/100] [Loss   0.57]\n",
            "[Epoch  90/100] [Loss:       3.17] [errors:     40]\n",
            "[Epoch  90/100] [Loss   8.39]\n",
            "[Epoch  91/100] [Loss:       2.73] [errors:     39]\n",
            "[Epoch  91/100] [Loss   1.51]\n",
            "[Epoch  92/100] [Loss:       1.97] [errors:     19]\n",
            "[Epoch  92/100] [Loss   0.34]\n",
            "[Epoch  93/100] [Loss:       1.59] [errors:     15]\n",
            "[Epoch  93/100] [Loss   0.18]\n",
            "[Epoch  94/100] [Loss:       1.56] [errors:     15]\n",
            "[Epoch  94/100] [Loss   0.10]\n",
            "[Epoch  95/100] [Loss:       1.54] [errors:     13]\n",
            "[Epoch  95/100] [Loss   0.09]\n",
            "[Epoch  96/100] [Loss:       1.58] [errors:     14]\n",
            "[Epoch  96/100] [Loss   0.07]\n",
            "[Epoch  97/100] [Loss:       1.61] [errors:     13]\n",
            "[Epoch  97/100] [Loss   0.07]\n",
            "[Epoch  98/100] [Loss:       1.62] [errors:     13]\n",
            "[Epoch  98/100] [Loss   0.06]\n",
            "[Epoch  99/100] [Loss:       1.58] [errors:     12]\n",
            "[Epoch  99/100] [Loss   0.06]\n",
            "[Epoch 100/100] [Loss:       1.62] [errors:     12]\n",
            "[Epoch 100/100] [Loss   0.05]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7xHg4CmL6BW",
        "outputId": "cf9c0328-5878-48f4-c42a-10b9e7565d5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "errors = 0\n",
        "for batch in testing_dataloader:\n",
        "    batch = Variable(batch.type(Tensor))\n",
        "    generated = autoencoder(batch)\n",
        "    res = generated.round()\n",
        "    diff = torch.abs(res - batch).view(1, 1, -1)[0][0].cpu().detach().numpy()\n",
        "    bad_diffs = diff[diff > 0.5]\n",
        "    errors += len(bad_diffs)\n",
        "print(\"total number of bad digits: {}\".format(errors))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total number of bad digits: 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0I5dbr5cGlQ"
      },
      "source": [
        "#############################\n",
        "### Generator Model ###\n",
        "#############################\n",
        "\n",
        "# Output should be 64 * 28\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(opt.latent_dim, 64 * 16),\n",
        "            nn.BatchNorm1d(64 * 16, eps=0.001, momentum=0.01),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(64 * 16, 64 * 28),\n",
        "            nn.BatchNorm1d(64 * 28, eps=0.001, momentum=0.01),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5oIATq7r9ek"
      },
      "source": [
        "###########################\n",
        "### Discriminator Model ###\n",
        "###########################\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        ndf = 16\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(2 * feature_size, feature_size),\n",
        "            # input is (nc) x 64 x 64\n",
        "            nn.Conv1d(1, ndf, 8, 4, 1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        \n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv1d(ndf, ndf * 2, 8, 4, 1),\n",
        "            nn.BatchNorm1d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        \n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv1d(ndf * 2, ndf * 4, 8, 4, 1),\n",
        "            nn.BatchNorm1d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        \n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv1d(ndf * 4, ndf * 8, 8, 4, 1),\n",
        "            nn.BatchNorm1d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        \n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv1d(ndf * 8, 1, 3, 1, 0),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_mean = torch.mean(x, 0).repeat(x.shape[0], 1)\n",
        "        x = torch.cat((x, x_mean), 1)\n",
        "        return self.model(x.view(-1, 1, 2 * feature_size)).view(-1, 1)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7inVoiMu9QS_",
        "tags": [],
        "outputId": "3f37e91d-5664-401a-fd19-351fecd5955f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "############################\n",
        "### Model Initialization ###\n",
        "############################\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "Tensor = torch.FloatTensor\n",
        "\n",
        "one = torch.FloatTensor([1])\n",
        "mone = one * -1\n",
        "\n",
        "if opt.cuda:\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    one = one.cuda()\n",
        "    mone = mone.cuda()\n",
        "    Tensor = torch.cuda.FloatTensor\n",
        "\n",
        "# generator_params = [{'params': generator.parameters()}, {'params': autoencoder.decoder.parameters(), 'lr': 1e-4}]\n",
        "\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr)\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr)\n",
        "\n",
        "generator.apply(weightsInit)\n",
        "discriminator.apply(weightsInit)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (model): Sequential(\n",
              "    (0): Linear(in_features=2142, out_features=1071, bias=True)\n",
              "    (1): Conv1d(1, 16, kernel_size=(8,), stride=(4,), padding=(1,))\n",
              "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (3): Conv1d(16, 32, kernel_size=(8,), stride=(4,), padding=(1,))\n",
              "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (6): Conv1d(32, 64, kernel_size=(8,), stride=(4,), padding=(1,))\n",
              "    (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (9): Conv1d(64, 128, kernel_size=(8,), stride=(4,), padding=(1,))\n",
              "    (10): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (12): Conv1d(128, 1, kernel_size=(3,), stride=(1,))\n",
              "    (13): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e83EKUAC3Edo",
        "outputId": "71b6f469-26d3-4053-820c-907bc0aba0ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if True:\n",
        "    batches_done = 0\n",
        "\n",
        "    discriminator.train()\n",
        "    generator.train()\n",
        "\n",
        "    gen_iterations = 0\n",
        "    for epoch in range(opt.n_epochs):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        for batch in training_dataloader:\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "            batch = Variable(batch.type(Tensor))\n",
        "\n",
        "            for dp in discriminator.parameters():\n",
        "                dp.requires_grad = True\n",
        "\n",
        "            if gen_iterations < 25 or gen_iterations % 500 == 0:\n",
        "                n_critic = 100\n",
        "            else:\n",
        "                n_critic = opt.n_critic\n",
        "\n",
        "            for _ in range(opt.n_critic):\n",
        "                for dp in discriminator.parameters():\n",
        "                    dp.data.clamp_(-opt.clamp, opt.clamp)\n",
        "\n",
        "                # reset gradients of discriminator\n",
        "                optimizer_D.zero_grad()\n",
        "\n",
        "                loss_D_real = torch.mean(discriminator(batch), dim=0)\n",
        "                loss_D_real.backward(one)\n",
        "\n",
        "                # Sample noise as generator input\n",
        "                z = torch.randn(batch.shape[0], opt.latent_dim, device=device)\n",
        "                # Generate a batch of images\n",
        "                fake_batch = torch.squeeze(autoencoder.decode(generator(z)))\n",
        "\n",
        "                # Error\n",
        "                loss_D_fake = torch.mean(discriminator(fake_batch.detach()), dim=0)\n",
        "                loss_D_fake.backward(mone)\n",
        "\n",
        "                # Optimizer stepz\n",
        "                optimizer_D.step()\n",
        "\n",
        "            # -----------------\n",
        "            #  Train Generator\n",
        "            # -----------------\n",
        "\n",
        "            for dp in discriminator.parameters():\n",
        "                dp.requires_grad = False\n",
        "\n",
        "            optimizer_G.zero_grad()\n",
        "\n",
        "            # Sample noise as generator input\n",
        "            z = torch.randn(batch.shape[0], opt.latent_dim, device=device)\n",
        "\n",
        "            # Generate a batch of images\n",
        "            fake_batch = torch.squeeze(autoencoder.decode(generator(z).unsqueeze(dim=2)))\n",
        "\n",
        "            # uncomment if there is no autoencoder\n",
        "            loss_G = torch.mean(discriminator(fake_batch), dim=0)\n",
        "            loss_G.backward(one)\n",
        "            optimizer_G.step()\n",
        "            batches_done += 1\n",
        "\n",
        "            if batches_done % 100 == 0:\n",
        "                print('[Epoch {:3d}/{:3d}] [Batch {:3d}/{:3d}] [D loss: {:.5f}] [G loss: {:.5f}]'.format(epoch + 1, opt.n_epochs, batches_done % len(training_dataloader), len(training_dataloader), loss_D_real.item() + loss_D_fake.item(), loss_G.item()))\n",
        "\n",
        "        print('[Epoch {:3d}/{:3d}] [Time: {:.2f}] [D loss: {:.5f}] [G loss: {:.5f}]'.format(epoch + 1, opt.n_epochs, time.time() - epoch_start, loss_D_real.item() + loss_D_fake.item(), loss_G.item()))\n",
        "\n",
        "    torch.save(generator.state_dict(), opt.expPATH + '/generator.model')\n",
        "    torch.save(discriminator.state_dict(), opt.expPATH + '/discriminator.model')\n",
        "else:\n",
        "    generator.load_state_dict(torch.load(opt.expPATH + '/generator.model'))\n",
        "    discriminator.load_state_dict(torch.load(opt.expPATH + '/discriminator.model'))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch   1/100] [Batch 100/582] [D loss: 1.00000] [G loss: 0.51010]\n",
            "[Epoch   1/100] [Batch 200/582] [D loss: 1.00000] [G loss: 0.51009]\n",
            "[Epoch   1/100] [Batch 300/582] [D loss: 1.00000] [G loss: 0.51009]\n",
            "[Epoch   1/100] [Batch 400/582] [D loss: 1.00000] [G loss: 0.51009]\n",
            "[Epoch   1/100] [Batch 500/582] [D loss: 1.00000] [G loss: 0.51009]\n",
            "[Epoch   1/100] [Time: 27.50] [D loss: 1.00000] [G loss: 0.51008]\n",
            "[Epoch   2/100] [Batch  18/582] [D loss: 1.00000] [G loss: 0.51009]\n",
            "[Epoch   2/100] [Batch 118/582] [D loss: 1.00002] [G loss: 0.51009]\n",
            "[Epoch   2/100] [Batch 218/582] [D loss: 1.00001] [G loss: 0.51010]\n",
            "[Epoch   2/100] [Batch 318/582] [D loss: 0.99993] [G loss: 0.51001]\n",
            "[Epoch   2/100] [Batch 418/582] [D loss: 1.00000] [G loss: 0.51008]\n",
            "[Epoch   2/100] [Batch 518/582] [D loss: 1.00000] [G loss: 0.51008]\n",
            "[Epoch   2/100] [Time: 27.44] [D loss: 1.00000] [G loss: 0.51009]\n",
            "[Epoch   3/100] [Batch  36/582] [D loss: 1.00002] [G loss: 0.51010]\n",
            "[Epoch   3/100] [Batch 136/582] [D loss: 1.00000] [G loss: 0.51009]\n",
            "[Epoch   3/100] [Batch 236/582] [D loss: 1.00000] [G loss: 0.51009]\n",
            "[Epoch   3/100] [Batch 336/582] [D loss: 1.00000] [G loss: 0.51009]\n",
            "[Epoch   3/100] [Batch 436/582] [D loss: 1.00000] [G loss: 0.51009]\n",
            "[Epoch   3/100] [Batch 536/582] [D loss: 0.99999] [G loss: 0.51009]\n",
            "[Epoch   3/100] [Time: 24.13] [D loss: 0.99997] [G loss: 0.51009]\n",
            "[Epoch   4/100] [Batch  54/582] [D loss: 1.00001] [G loss: 0.51009]\n",
            "[Epoch   4/100] [Batch 154/582] [D loss: 1.00000] [G loss: 0.51009]\n",
            "[Epoch   4/100] [Batch 254/582] [D loss: 1.00000] [G loss: 0.51009]\n",
            "[Epoch   4/100] [Batch 354/582] [D loss: 1.00000] [G loss: 0.51009]\n",
            "[Epoch   4/100] [Batch 454/582] [D loss: 0.99999] [G loss: 0.51009]\n",
            "[Epoch   4/100] [Batch 554/582] [D loss: 1.00000] [G loss: 0.51009]\n",
            "[Epoch   4/100] [Time: 27.87] [D loss: 1.00000] [G loss: 0.51009]\n",
            "[Epoch   5/100] [Batch  72/582] [D loss: 1.00000] [G loss: 0.51009]\n",
            "[Epoch   5/100] [Batch 172/582] [D loss: 1.00001] [G loss: 0.51011]\n",
            "[Epoch   5/100] [Batch 272/582] [D loss: 1.00004] [G loss: 0.51010]\n",
            "[Epoch   5/100] [Batch 372/582] [D loss: 0.99999] [G loss: 0.51009]\n",
            "[Epoch   5/100] [Batch 472/582] [D loss: 0.99999] [G loss: 0.51008]\n",
            "[Epoch   5/100] [Batch 572/582] [D loss: 1.00001] [G loss: 0.51010]\n",
            "[Epoch   5/100] [Time: 27.46] [D loss: 1.00025] [G loss: 0.51012]\n",
            "[Epoch   6/100] [Batch  90/582] [D loss: 0.99999] [G loss: 0.51007]\n",
            "[Epoch   6/100] [Batch 190/582] [D loss: 1.00000] [G loss: 0.51009]\n",
            "[Epoch   6/100] [Batch 290/582] [D loss: 1.00000] [G loss: 0.51009]\n",
            "[Epoch   6/100] [Batch 390/582] [D loss: 1.00000] [G loss: 0.51009]\n",
            "[Epoch   6/100] [Batch 490/582] [D loss: 1.00000] [G loss: 0.51009]\n",
            "[Epoch   6/100] [Time: 27.83] [D loss: 1.00001] [G loss: 0.51009]\n",
            "[Epoch   7/100] [Batch   8/582] [D loss: 1.00000] [G loss: 0.51009]\n",
            "[Epoch   7/100] [Batch 108/582] [D loss: 0.99999] [G loss: 0.51009]\n",
            "[Epoch   7/100] [Batch 208/582] [D loss: 1.00000] [G loss: 0.51009]\n",
            "[Epoch   7/100] [Batch 308/582] [D loss: 1.00000] [G loss: 0.51009]\n",
            "[Epoch   7/100] [Batch 408/582] [D loss: 0.99998] [G loss: 0.51009]\n",
            "[Epoch   7/100] [Batch 508/582] [D loss: 0.99996] [G loss: 0.50997]\n",
            "[Epoch   7/100] [Time: 27.38] [D loss: 0.99983] [G loss: 0.51006]\n",
            "[Epoch   8/100] [Batch  26/582] [D loss: 1.00001] [G loss: 0.51007]\n",
            "[Epoch   8/100] [Batch 126/582] [D loss: 1.00000] [G loss: 0.51009]\n",
            "[Epoch   8/100] [Batch 226/582] [D loss: 1.00000] [G loss: 0.51009]\n",
            "[Epoch   8/100] [Batch 326/582] [D loss: 0.99999] [G loss: 0.51009]\n",
            "[Epoch   8/100] [Batch 426/582] [D loss: 1.00000] [G loss: 0.51010]\n",
            "[Epoch   8/100] [Batch 526/582] [D loss: 1.00000] [G loss: 0.51009]\n",
            "[Epoch   8/100] [Time: 27.66] [D loss: 0.99996] [G loss: 0.51007]\n",
            "[Epoch   9/100] [Batch  44/582] [D loss: 0.99999] [G loss: 0.51007]\n",
            "[Epoch   9/100] [Batch 144/582] [D loss: 1.00001] [G loss: 0.51008]\n",
            "[Epoch   9/100] [Batch 244/582] [D loss: 0.99850] [G loss: 0.50916]\n",
            "[Epoch   9/100] [Batch 344/582] [D loss: 0.99946] [G loss: 0.50965]\n",
            "[Epoch   9/100] [Batch 444/582] [D loss: 1.00003] [G loss: 0.50995]\n",
            "[Epoch   9/100] [Batch 544/582] [D loss: 0.99994] [G loss: 0.50996]\n",
            "[Epoch   9/100] [Time: 27.39] [D loss: 1.00006] [G loss: 0.51010]\n",
            "[Epoch  10/100] [Batch  62/582] [D loss: 0.99984] [G loss: 0.50973]\n",
            "[Epoch  10/100] [Batch 162/582] [D loss: 0.99980] [G loss: 0.51003]\n",
            "[Epoch  10/100] [Batch 262/582] [D loss: 0.99869] [G loss: 0.50952]\n",
            "[Epoch  10/100] [Batch 362/582] [D loss: 0.99971] [G loss: 0.50985]\n",
            "[Epoch  10/100] [Batch 462/582] [D loss: 0.99991] [G loss: 0.51002]\n",
            "[Epoch  10/100] [Batch 562/582] [D loss: 0.99948] [G loss: 0.50989]\n",
            "[Epoch  10/100] [Time: 27.52] [D loss: 1.00014] [G loss: 0.51017]\n",
            "[Epoch  11/100] [Batch  80/582] [D loss: 0.99949] [G loss: 0.50992]\n",
            "[Epoch  11/100] [Batch 180/582] [D loss: 0.99926] [G loss: 0.50962]\n",
            "[Epoch  11/100] [Batch 280/582] [D loss: 0.99733] [G loss: 0.50933]\n",
            "[Epoch  11/100] [Batch 380/582] [D loss: 0.99969] [G loss: 0.50984]\n",
            "[Epoch  11/100] [Batch 480/582] [D loss: 0.99941] [G loss: 0.50981]\n",
            "[Epoch  11/100] [Batch 580/582] [D loss: 0.99975] [G loss: 0.50794]\n",
            "[Epoch  11/100] [Time: 27.39] [D loss: 1.00190] [G loss: 0.50994]\n",
            "[Epoch  12/100] [Batch  98/582] [D loss: 1.00006] [G loss: 0.51016]\n",
            "[Epoch  12/100] [Batch 198/582] [D loss: 1.00098] [G loss: 0.50947]\n",
            "[Epoch  12/100] [Batch 298/582] [D loss: 0.99611] [G loss: 0.50816]\n",
            "[Epoch  12/100] [Batch 398/582] [D loss: 0.99913] [G loss: 0.50862]\n",
            "[Epoch  12/100] [Batch 498/582] [D loss: 0.99832] [G loss: 0.50938]\n",
            "[Epoch  12/100] [Time: 23.12] [D loss: 0.99857] [G loss: 0.50892]\n",
            "[Epoch  13/100] [Batch  16/582] [D loss: 0.99891] [G loss: 0.50966]\n",
            "[Epoch  13/100] [Batch 116/582] [D loss: 0.99920] [G loss: 0.50942]\n",
            "[Epoch  13/100] [Batch 216/582] [D loss: 0.99837] [G loss: 0.50637]\n",
            "[Epoch  13/100] [Batch 316/582] [D loss: 0.99983] [G loss: 0.50908]\n",
            "[Epoch  13/100] [Batch 416/582] [D loss: 0.99840] [G loss: 0.50918]\n",
            "[Epoch  13/100] [Batch 516/582] [D loss: 1.00071] [G loss: 0.51013]\n",
            "[Epoch  13/100] [Time: 27.21] [D loss: 0.99864] [G loss: 0.50859]\n",
            "[Epoch  14/100] [Batch  34/582] [D loss: 0.99927] [G loss: 0.50954]\n",
            "[Epoch  14/100] [Batch 134/582] [D loss: 0.99544] [G loss: 0.50835]\n",
            "[Epoch  14/100] [Batch 234/582] [D loss: 0.99732] [G loss: 0.50837]\n",
            "[Epoch  14/100] [Batch 334/582] [D loss: 0.99807] [G loss: 0.50875]\n",
            "[Epoch  14/100] [Batch 434/582] [D loss: 0.99903] [G loss: 0.50887]\n",
            "[Epoch  14/100] [Batch 534/582] [D loss: 1.00014] [G loss: 0.50851]\n",
            "[Epoch  14/100] [Time: 27.39] [D loss: 0.99711] [G loss: 0.50891]\n",
            "[Epoch  15/100] [Batch  52/582] [D loss: 0.99274] [G loss: 0.50835]\n",
            "[Epoch  15/100] [Batch 152/582] [D loss: 0.99955] [G loss: 0.50875]\n",
            "[Epoch  15/100] [Batch 252/582] [D loss: 0.99831] [G loss: 0.50852]\n",
            "[Epoch  15/100] [Batch 352/582] [D loss: 0.99872] [G loss: 0.50905]\n",
            "[Epoch  15/100] [Batch 452/582] [D loss: 0.99975] [G loss: 0.50784]\n",
            "[Epoch  15/100] [Batch 552/582] [D loss: 0.99540] [G loss: 0.50935]\n",
            "[Epoch  15/100] [Time: 27.10] [D loss: 0.99908] [G loss: 0.50976]\n",
            "[Epoch  16/100] [Batch  70/582] [D loss: 0.99832] [G loss: 0.50578]\n",
            "[Epoch  16/100] [Batch 170/582] [D loss: 0.99831] [G loss: 0.50645]\n",
            "[Epoch  16/100] [Batch 270/582] [D loss: 1.00041] [G loss: 0.50892]\n",
            "[Epoch  16/100] [Batch 370/582] [D loss: 1.00027] [G loss: 0.50927]\n",
            "[Epoch  16/100] [Batch 470/582] [D loss: 0.99510] [G loss: 0.50875]\n",
            "[Epoch  16/100] [Batch 570/582] [D loss: 0.99992] [G loss: 0.50908]\n",
            "[Epoch  16/100] [Time: 27.10] [D loss: 0.99949] [G loss: 0.50884]\n",
            "[Epoch  17/100] [Batch  88/582] [D loss: 0.99541] [G loss: 0.50797]\n",
            "[Epoch  17/100] [Batch 188/582] [D loss: 1.00171] [G loss: 0.50887]\n",
            "[Epoch  17/100] [Batch 288/582] [D loss: 0.99597] [G loss: 0.50487]\n",
            "[Epoch  17/100] [Batch 388/582] [D loss: 0.99892] [G loss: 0.50458]\n",
            "[Epoch  17/100] [Batch 488/582] [D loss: 0.99756] [G loss: 0.50978]\n",
            "[Epoch  17/100] [Time: 27.51] [D loss: 1.00018] [G loss: 0.50984]\n",
            "[Epoch  18/100] [Batch   6/582] [D loss: 1.00046] [G loss: 0.50866]\n",
            "[Epoch  18/100] [Batch 106/582] [D loss: 0.99919] [G loss: 0.50937]\n",
            "[Epoch  18/100] [Batch 206/582] [D loss: 0.99634] [G loss: 0.50695]\n",
            "[Epoch  18/100] [Batch 306/582] [D loss: 0.99843] [G loss: 0.50962]\n",
            "[Epoch  18/100] [Batch 406/582] [D loss: 0.99878] [G loss: 0.50861]\n",
            "[Epoch  18/100] [Batch 506/582] [D loss: 0.99767] [G loss: 0.50828]\n",
            "[Epoch  18/100] [Time: 27.33] [D loss: 1.00023] [G loss: 0.50890]\n",
            "[Epoch  19/100] [Batch  24/582] [D loss: 0.99902] [G loss: 0.50881]\n",
            "[Epoch  19/100] [Batch 124/582] [D loss: 0.99948] [G loss: 0.50913]\n",
            "[Epoch  19/100] [Batch 224/582] [D loss: 0.99913] [G loss: 0.50770]\n",
            "[Epoch  19/100] [Batch 324/582] [D loss: 0.99466] [G loss: 0.50718]\n",
            "[Epoch  19/100] [Batch 424/582] [D loss: 1.00068] [G loss: 0.50849]\n",
            "[Epoch  19/100] [Batch 524/582] [D loss: 0.99824] [G loss: 0.50767]\n",
            "[Epoch  19/100] [Time: 27.14] [D loss: 0.99971] [G loss: 0.50692]\n",
            "[Epoch  20/100] [Batch  42/582] [D loss: 0.99958] [G loss: 0.50921]\n",
            "[Epoch  20/100] [Batch 142/582] [D loss: 1.00181] [G loss: 0.50750]\n",
            "[Epoch  20/100] [Batch 242/582] [D loss: 0.99850] [G loss: 0.50816]\n",
            "[Epoch  20/100] [Batch 342/582] [D loss: 0.99823] [G loss: 0.50743]\n",
            "[Epoch  20/100] [Batch 442/582] [D loss: 0.99665] [G loss: 0.50647]\n",
            "[Epoch  20/100] [Batch 542/582] [D loss: 0.99823] [G loss: 0.50752]\n",
            "[Epoch  20/100] [Time: 21.08] [D loss: 1.00361] [G loss: 0.50469]\n",
            "[Epoch  21/100] [Batch  60/582] [D loss: 0.99809] [G loss: 0.50833]\n",
            "[Epoch  21/100] [Batch 160/582] [D loss: 0.99911] [G loss: 0.50910]\n",
            "[Epoch  21/100] [Batch 260/582] [D loss: 0.99594] [G loss: 0.50626]\n",
            "[Epoch  21/100] [Batch 360/582] [D loss: 0.99616] [G loss: 0.50345]\n",
            "[Epoch  21/100] [Batch 460/582] [D loss: 0.99831] [G loss: 0.50762]\n",
            "[Epoch  21/100] [Batch 560/582] [D loss: 0.99993] [G loss: 0.50705]\n",
            "[Epoch  21/100] [Time: 25.35] [D loss: 1.01191] [G loss: 0.50902]\n",
            "[Epoch  22/100] [Batch  78/582] [D loss: 0.99965] [G loss: 0.50908]\n",
            "[Epoch  22/100] [Batch 178/582] [D loss: 0.99720] [G loss: 0.50662]\n",
            "[Epoch  22/100] [Batch 278/582] [D loss: 0.99779] [G loss: 0.50873]\n",
            "[Epoch  22/100] [Batch 378/582] [D loss: 0.99432] [G loss: 0.50705]\n",
            "[Epoch  22/100] [Batch 478/582] [D loss: 1.00108] [G loss: 0.50943]\n",
            "[Epoch  22/100] [Batch 578/582] [D loss: 0.99916] [G loss: 0.50725]\n",
            "[Epoch  22/100] [Time: 24.80] [D loss: 0.99957] [G loss: 0.50842]\n",
            "[Epoch  23/100] [Batch  96/582] [D loss: 0.99757] [G loss: 0.49591]\n",
            "[Epoch  23/100] [Batch 196/582] [D loss: 1.00036] [G loss: 0.50790]\n",
            "[Epoch  23/100] [Batch 296/582] [D loss: 0.99886] [G loss: 0.50822]\n",
            "[Epoch  23/100] [Batch 396/582] [D loss: 1.00000] [G loss: 0.50575]\n",
            "[Epoch  23/100] [Batch 496/582] [D loss: 0.99678] [G loss: 0.50823]\n",
            "[Epoch  23/100] [Time: 27.54] [D loss: 0.99888] [G loss: 0.50946]\n",
            "[Epoch  24/100] [Batch  14/582] [D loss: 1.00058] [G loss: 0.50508]\n",
            "[Epoch  24/100] [Batch 114/582] [D loss: 1.00089] [G loss: 0.50714]\n",
            "[Epoch  24/100] [Batch 214/582] [D loss: 0.99796] [G loss: 0.50916]\n",
            "[Epoch  24/100] [Batch 314/582] [D loss: 0.99657] [G loss: 0.50606]\n",
            "[Epoch  24/100] [Batch 414/582] [D loss: 0.99431] [G loss: 0.50492]\n",
            "[Epoch  24/100] [Batch 514/582] [D loss: 0.99971] [G loss: 0.50784]\n",
            "[Epoch  24/100] [Time: 27.38] [D loss: 0.99874] [G loss: 0.50701]\n",
            "[Epoch  25/100] [Batch  32/582] [D loss: 0.99499] [G loss: 0.50231]\n",
            "[Epoch  25/100] [Batch 132/582] [D loss: 1.00067] [G loss: 0.50625]\n",
            "[Epoch  25/100] [Batch 232/582] [D loss: 0.99841] [G loss: 0.50305]\n",
            "[Epoch  25/100] [Batch 332/582] [D loss: 0.99993] [G loss: 0.50784]\n",
            "[Epoch  25/100] [Batch 432/582] [D loss: 0.99646] [G loss: 0.49810]\n",
            "[Epoch  25/100] [Batch 532/582] [D loss: 0.99590] [G loss: 0.50586]\n",
            "[Epoch  25/100] [Time: 27.23] [D loss: 0.99991] [G loss: 0.50907]\n",
            "[Epoch  26/100] [Batch  50/582] [D loss: 0.99848] [G loss: 0.50768]\n",
            "[Epoch  26/100] [Batch 150/582] [D loss: 0.99413] [G loss: 0.49829]\n",
            "[Epoch  26/100] [Batch 250/582] [D loss: 0.99280] [G loss: 0.50831]\n",
            "[Epoch  26/100] [Batch 350/582] [D loss: 0.99730] [G loss: 0.50773]\n",
            "[Epoch  26/100] [Batch 450/582] [D loss: 0.99876] [G loss: 0.50691]\n",
            "[Epoch  26/100] [Batch 550/582] [D loss: 0.99629] [G loss: 0.50885]\n",
            "[Epoch  26/100] [Time: 23.68] [D loss: 0.99999] [G loss: 0.50943]\n",
            "[Epoch  27/100] [Batch  68/582] [D loss: 1.00091] [G loss: 0.50821]\n",
            "[Epoch  27/100] [Batch 168/582] [D loss: 1.00002] [G loss: 0.50704]\n",
            "[Epoch  27/100] [Batch 268/582] [D loss: 0.99990] [G loss: 0.50303]\n",
            "[Epoch  27/100] [Batch 368/582] [D loss: 0.99971] [G loss: 0.50648]\n",
            "[Epoch  27/100] [Batch 468/582] [D loss: 0.99585] [G loss: 0.50925]\n",
            "[Epoch  27/100] [Batch 568/582] [D loss: 1.00534] [G loss: 0.50365]\n",
            "[Epoch  27/100] [Time: 23.62] [D loss: 0.99961] [G loss: 0.50958]\n",
            "[Epoch  28/100] [Batch  86/582] [D loss: 0.99876] [G loss: 0.50744]\n",
            "[Epoch  28/100] [Batch 186/582] [D loss: 1.00013] [G loss: 0.49984]\n",
            "[Epoch  28/100] [Batch 286/582] [D loss: 0.99054] [G loss: 0.50311]\n",
            "[Epoch  28/100] [Batch 386/582] [D loss: 1.00026] [G loss: 0.50696]\n",
            "[Epoch  28/100] [Batch 486/582] [D loss: 0.99450] [G loss: 0.50686]\n",
            "[Epoch  28/100] [Time: 26.38] [D loss: 0.99772] [G loss: 0.50743]\n",
            "[Epoch  29/100] [Batch   4/582] [D loss: 0.99958] [G loss: 0.49868]\n",
            "[Epoch  29/100] [Batch 104/582] [D loss: 0.99851] [G loss: 0.50805]\n",
            "[Epoch  29/100] [Batch 204/582] [D loss: 1.00201] [G loss: 0.50643]\n",
            "[Epoch  29/100] [Batch 304/582] [D loss: 0.99741] [G loss: 0.50750]\n",
            "[Epoch  29/100] [Batch 404/582] [D loss: 1.00019] [G loss: 0.50725]\n",
            "[Epoch  29/100] [Batch 504/582] [D loss: 0.99936] [G loss: 0.50449]\n",
            "[Epoch  29/100] [Time: 27.40] [D loss: 1.00025] [G loss: 0.50547]\n",
            "[Epoch  30/100] [Batch  22/582] [D loss: 0.99836] [G loss: 0.50850]\n",
            "[Epoch  30/100] [Batch 122/582] [D loss: 0.99603] [G loss: 0.50215]\n",
            "[Epoch  30/100] [Batch 222/582] [D loss: 1.00839] [G loss: 0.50719]\n",
            "[Epoch  30/100] [Batch 322/582] [D loss: 1.00563] [G loss: 0.50470]\n",
            "[Epoch  30/100] [Batch 422/582] [D loss: 0.99884] [G loss: 0.50888]\n",
            "[Epoch  30/100] [Batch 522/582] [D loss: 0.99917] [G loss: 0.50837]\n",
            "[Epoch  30/100] [Time: 26.74] [D loss: 0.99454] [G loss: 0.50463]\n",
            "[Epoch  31/100] [Batch  40/582] [D loss: 0.99616] [G loss: 0.50762]\n",
            "[Epoch  31/100] [Batch 140/582] [D loss: 0.99685] [G loss: 0.50775]\n",
            "[Epoch  31/100] [Batch 240/582] [D loss: 0.99686] [G loss: 0.50598]\n",
            "[Epoch  31/100] [Batch 340/582] [D loss: 0.99908] [G loss: 0.50631]\n",
            "[Epoch  31/100] [Batch 440/582] [D loss: 1.00849] [G loss: 0.50406]\n",
            "[Epoch  31/100] [Batch 540/582] [D loss: 0.99973] [G loss: 0.50871]\n",
            "[Epoch  31/100] [Time: 27.27] [D loss: 0.99962] [G loss: 0.50681]\n",
            "[Epoch  32/100] [Batch  58/582] [D loss: 1.01645] [G loss: 0.50869]\n",
            "[Epoch  32/100] [Batch 158/582] [D loss: 0.99631] [G loss: 0.50854]\n",
            "[Epoch  32/100] [Batch 258/582] [D loss: 0.99833] [G loss: 0.50535]\n",
            "[Epoch  32/100] [Batch 358/582] [D loss: 0.99990] [G loss: 0.50798]\n",
            "[Epoch  32/100] [Batch 458/582] [D loss: 1.00182] [G loss: 0.50167]\n",
            "[Epoch  32/100] [Batch 558/582] [D loss: 1.00290] [G loss: 0.50128]\n",
            "[Epoch  32/100] [Time: 27.29] [D loss: 1.00206] [G loss: 0.50342]\n",
            "[Epoch  33/100] [Batch  76/582] [D loss: 0.99461] [G loss: 0.50502]\n",
            "[Epoch  33/100] [Batch 176/582] [D loss: 0.99951] [G loss: 0.51026]\n",
            "[Epoch  33/100] [Batch 276/582] [D loss: 0.99760] [G loss: 0.50618]\n",
            "[Epoch  33/100] [Batch 376/582] [D loss: 0.99141] [G loss: 0.50424]\n",
            "[Epoch  33/100] [Batch 476/582] [D loss: 0.99765] [G loss: 0.50345]\n",
            "[Epoch  33/100] [Batch 576/582] [D loss: 1.00384] [G loss: 0.50185]\n",
            "[Epoch  33/100] [Time: 27.33] [D loss: 1.00909] [G loss: 0.50531]\n",
            "[Epoch  34/100] [Batch  94/582] [D loss: 0.99346] [G loss: 0.50067]\n",
            "[Epoch  34/100] [Batch 194/582] [D loss: 0.99813] [G loss: 0.50750]\n",
            "[Epoch  34/100] [Batch 294/582] [D loss: 0.99762] [G loss: 0.50603]\n",
            "[Epoch  34/100] [Batch 394/582] [D loss: 1.00065] [G loss: 0.50304]\n",
            "[Epoch  34/100] [Batch 494/582] [D loss: 0.99787] [G loss: 0.50477]\n",
            "[Epoch  34/100] [Time: 27.54] [D loss: 1.00000] [G loss: 0.50686]\n",
            "[Epoch  35/100] [Batch  12/582] [D loss: 0.99953] [G loss: 0.50808]\n",
            "[Epoch  35/100] [Batch 112/582] [D loss: 0.99950] [G loss: 0.50844]\n",
            "[Epoch  35/100] [Batch 212/582] [D loss: 0.99703] [G loss: 0.50246]\n",
            "[Epoch  35/100] [Batch 312/582] [D loss: 0.99454] [G loss: 0.50685]\n",
            "[Epoch  35/100] [Batch 412/582] [D loss: 0.99509] [G loss: 0.50843]\n",
            "[Epoch  35/100] [Batch 512/582] [D loss: 0.99938] [G loss: 0.50606]\n",
            "[Epoch  35/100] [Time: 27.19] [D loss: 1.01475] [G loss: 0.50817]\n",
            "[Epoch  36/100] [Batch  30/582] [D loss: 0.99942] [G loss: 0.50131]\n",
            "[Epoch  36/100] [Batch 130/582] [D loss: 0.99881] [G loss: 0.50833]\n",
            "[Epoch  36/100] [Batch 230/582] [D loss: 0.99758] [G loss: 0.50760]\n",
            "[Epoch  36/100] [Batch 330/582] [D loss: 0.99423] [G loss: 0.50763]\n",
            "[Epoch  36/100] [Batch 430/582] [D loss: 1.00500] [G loss: 0.50661]\n",
            "[Epoch  36/100] [Batch 530/582] [D loss: 0.99867] [G loss: 0.50522]\n",
            "[Epoch  36/100] [Time: 27.33] [D loss: 0.99742] [G loss: 0.50645]\n",
            "[Epoch  37/100] [Batch  48/582] [D loss: 0.99171] [G loss: 0.50378]\n",
            "[Epoch  37/100] [Batch 148/582] [D loss: 0.99688] [G loss: 0.50830]\n",
            "[Epoch  37/100] [Batch 248/582] [D loss: 0.99783] [G loss: 0.50847]\n",
            "[Epoch  37/100] [Batch 348/582] [D loss: 1.00045] [G loss: 0.50421]\n",
            "[Epoch  37/100] [Batch 448/582] [D loss: 1.00073] [G loss: 0.50635]\n",
            "[Epoch  37/100] [Batch 548/582] [D loss: 1.01055] [G loss: 0.50600]\n",
            "[Epoch  37/100] [Time: 27.78] [D loss: 0.99383] [G loss: 0.50088]\n",
            "[Epoch  38/100] [Batch  66/582] [D loss: 1.00724] [G loss: 0.50672]\n",
            "[Epoch  38/100] [Batch 166/582] [D loss: 0.99532] [G loss: 0.50393]\n",
            "[Epoch  38/100] [Batch 266/582] [D loss: 1.00072] [G loss: 0.50288]\n",
            "[Epoch  38/100] [Batch 366/582] [D loss: 0.99664] [G loss: 0.50683]\n",
            "[Epoch  38/100] [Batch 466/582] [D loss: 0.99877] [G loss: 0.50828]\n",
            "[Epoch  38/100] [Batch 566/582] [D loss: 0.99397] [G loss: 0.50588]\n",
            "[Epoch  38/100] [Time: 26.57] [D loss: 1.00062] [G loss: 0.51019]\n",
            "[Epoch  39/100] [Batch  84/582] [D loss: 0.99493] [G loss: 0.50823]\n",
            "[Epoch  39/100] [Batch 184/582] [D loss: 0.99965] [G loss: 0.50809]\n",
            "[Epoch  39/100] [Batch 284/582] [D loss: 0.99444] [G loss: 0.50565]\n",
            "[Epoch  39/100] [Batch 384/582] [D loss: 0.99563] [G loss: 0.50353]\n",
            "[Epoch  39/100] [Batch 484/582] [D loss: 0.99617] [G loss: 0.50480]\n",
            "[Epoch  39/100] [Time: 22.64] [D loss: 1.00085] [G loss: 0.50907]\n",
            "[Epoch  40/100] [Batch   2/582] [D loss: 0.99768] [G loss: 0.50539]\n",
            "[Epoch  40/100] [Batch 102/582] [D loss: 0.99920] [G loss: 0.50709]\n",
            "[Epoch  40/100] [Batch 202/582] [D loss: 0.99849] [G loss: 0.50322]\n",
            "[Epoch  40/100] [Batch 302/582] [D loss: 1.00565] [G loss: 0.50259]\n",
            "[Epoch  40/100] [Batch 402/582] [D loss: 0.99893] [G loss: 0.50671]\n",
            "[Epoch  40/100] [Batch 502/582] [D loss: 0.99860] [G loss: 0.50664]\n",
            "[Epoch  40/100] [Time: 27.39] [D loss: 0.99785] [G loss: 0.50801]\n",
            "[Epoch  41/100] [Batch  20/582] [D loss: 0.99782] [G loss: 0.50720]\n",
            "[Epoch  41/100] [Batch 120/582] [D loss: 0.99722] [G loss: 0.50856]\n",
            "[Epoch  41/100] [Batch 220/582] [D loss: 1.00153] [G loss: 0.50288]\n",
            "[Epoch  41/100] [Batch 320/582] [D loss: 0.99430] [G loss: 0.50431]\n",
            "[Epoch  41/100] [Batch 420/582] [D loss: 0.99611] [G loss: 0.50819]\n",
            "[Epoch  41/100] [Batch 520/582] [D loss: 0.99619] [G loss: 0.50289]\n",
            "[Epoch  41/100] [Time: 27.34] [D loss: 1.00781] [G loss: 0.50489]\n",
            "[Epoch  42/100] [Batch  38/582] [D loss: 1.00681] [G loss: 0.50596]\n",
            "[Epoch  42/100] [Batch 138/582] [D loss: 0.99875] [G loss: 0.50665]\n",
            "[Epoch  42/100] [Batch 238/582] [D loss: 0.99969] [G loss: 0.50584]\n",
            "[Epoch  42/100] [Batch 338/582] [D loss: 0.99897] [G loss: 0.50513]\n",
            "[Epoch  42/100] [Batch 438/582] [D loss: 0.99893] [G loss: 0.50538]\n",
            "[Epoch  42/100] [Batch 538/582] [D loss: 0.99587] [G loss: 0.50455]\n",
            "[Epoch  42/100] [Time: 27.28] [D loss: 0.99835] [G loss: 0.50713]\n",
            "[Epoch  43/100] [Batch  56/582] [D loss: 1.00135] [G loss: 0.50498]\n",
            "[Epoch  43/100] [Batch 156/582] [D loss: 1.00563] [G loss: 0.50566]\n",
            "[Epoch  43/100] [Batch 256/582] [D loss: 0.99947] [G loss: 0.50515]\n",
            "[Epoch  43/100] [Batch 356/582] [D loss: 0.99598] [G loss: 0.50669]\n",
            "[Epoch  43/100] [Batch 456/582] [D loss: 0.99517] [G loss: 0.50459]\n",
            "[Epoch  43/100] [Batch 556/582] [D loss: 1.00129] [G loss: 0.49606]\n",
            "[Epoch  43/100] [Time: 27.44] [D loss: 0.99987] [G loss: 0.49793]\n",
            "[Epoch  44/100] [Batch  74/582] [D loss: 1.01216] [G loss: 0.50824]\n",
            "[Epoch  44/100] [Batch 174/582] [D loss: 0.99388] [G loss: 0.50777]\n",
            "[Epoch  44/100] [Batch 274/582] [D loss: 0.99916] [G loss: 0.50792]\n",
            "[Epoch  44/100] [Batch 374/582] [D loss: 0.99994] [G loss: 0.50532]\n",
            "[Epoch  44/100] [Batch 474/582] [D loss: 1.00633] [G loss: 0.50354]\n",
            "[Epoch  44/100] [Batch 574/582] [D loss: 1.00016] [G loss: 0.50619]\n",
            "[Epoch  44/100] [Time: 27.57] [D loss: 0.99753] [G loss: 0.50673]\n",
            "[Epoch  45/100] [Batch  92/582] [D loss: 1.00590] [G loss: 0.50322]\n",
            "[Epoch  45/100] [Batch 192/582] [D loss: 0.99795] [G loss: 0.50337]\n",
            "[Epoch  45/100] [Batch 292/582] [D loss: 0.99997] [G loss: 0.50788]\n",
            "[Epoch  45/100] [Batch 392/582] [D loss: 0.99878] [G loss: 0.50401]\n",
            "[Epoch  45/100] [Batch 492/582] [D loss: 0.99681] [G loss: 0.50428]\n",
            "[Epoch  45/100] [Time: 27.43] [D loss: 0.99955] [G loss: 0.51003]\n",
            "[Epoch  46/100] [Batch  10/582] [D loss: 0.99700] [G loss: 0.50788]\n",
            "[Epoch  46/100] [Batch 110/582] [D loss: 0.99855] [G loss: 0.50839]\n",
            "[Epoch  46/100] [Batch 210/582] [D loss: 1.00134] [G loss: 0.50908]\n",
            "[Epoch  46/100] [Batch 310/582] [D loss: 0.99728] [G loss: 0.50720]\n",
            "[Epoch  46/100] [Batch 410/582] [D loss: 1.00210] [G loss: 0.50336]\n",
            "[Epoch  46/100] [Batch 510/582] [D loss: 0.99899] [G loss: 0.50804]\n",
            "[Epoch  46/100] [Time: 27.43] [D loss: 0.99105] [G loss: 0.50377]\n",
            "[Epoch  47/100] [Batch  28/582] [D loss: 0.99912] [G loss: 0.50405]\n",
            "[Epoch  47/100] [Batch 128/582] [D loss: 0.99765] [G loss: 0.50642]\n",
            "[Epoch  47/100] [Batch 228/582] [D loss: 0.99958] [G loss: 0.50551]\n",
            "[Epoch  47/100] [Batch 328/582] [D loss: 0.99376] [G loss: 0.50047]\n",
            "[Epoch  47/100] [Batch 428/582] [D loss: 1.00002] [G loss: 0.50653]\n",
            "[Epoch  47/100] [Batch 528/582] [D loss: 0.99761] [G loss: 0.50418]\n",
            "[Epoch  47/100] [Time: 27.33] [D loss: 0.99997] [G loss: 0.50930]\n",
            "[Epoch  48/100] [Batch  46/582] [D loss: 0.99554] [G loss: 0.50637]\n",
            "[Epoch  48/100] [Batch 146/582] [D loss: 0.99839] [G loss: 0.50070]\n",
            "[Epoch  48/100] [Batch 246/582] [D loss: 1.00397] [G loss: 0.50291]\n",
            "[Epoch  48/100] [Batch 346/582] [D loss: 0.99517] [G loss: 0.50591]\n",
            "[Epoch  48/100] [Batch 446/582] [D loss: 0.99056] [G loss: 0.50221]\n",
            "[Epoch  48/100] [Batch 546/582] [D loss: 0.99712] [G loss: 0.50398]\n",
            "[Epoch  48/100] [Time: 27.64] [D loss: 1.00090] [G loss: 0.50843]\n",
            "[Epoch  49/100] [Batch  64/582] [D loss: 0.99880] [G loss: 0.50120]\n",
            "[Epoch  49/100] [Batch 164/582] [D loss: 0.99649] [G loss: 0.50240]\n",
            "[Epoch  49/100] [Batch 264/582] [D loss: 1.01658] [G loss: 0.50920]\n",
            "[Epoch  49/100] [Batch 364/582] [D loss: 0.99403] [G loss: 0.50533]\n",
            "[Epoch  49/100] [Batch 464/582] [D loss: 1.00435] [G loss: 0.50650]\n",
            "[Epoch  49/100] [Batch 564/582] [D loss: 1.00048] [G loss: 0.50742]\n",
            "[Epoch  49/100] [Time: 27.20] [D loss: 0.99824] [G loss: 0.50854]\n",
            "[Epoch  50/100] [Batch  82/582] [D loss: 0.99815] [G loss: 0.50345]\n",
            "[Epoch  50/100] [Batch 182/582] [D loss: 0.99782] [G loss: 0.50342]\n",
            "[Epoch  50/100] [Batch 282/582] [D loss: 1.00022] [G loss: 0.49958]\n",
            "[Epoch  50/100] [Batch 382/582] [D loss: 0.99601] [G loss: 0.50428]\n",
            "[Epoch  50/100] [Batch 482/582] [D loss: 0.99994] [G loss: 0.50727]\n",
            "[Epoch  50/100] [Batch   0/582] [D loss: 1.00082] [G loss: 0.50869]\n",
            "[Epoch  50/100] [Time: 27.32] [D loss: 1.00082] [G loss: 0.50869]\n",
            "[Epoch  51/100] [Batch 100/582] [D loss: 1.01217] [G loss: 0.50632]\n",
            "[Epoch  51/100] [Batch 200/582] [D loss: 1.00377] [G loss: 0.50222]\n",
            "[Epoch  51/100] [Batch 300/582] [D loss: 0.99534] [G loss: 0.49965]\n",
            "[Epoch  51/100] [Batch 400/582] [D loss: 1.00852] [G loss: 0.50607]\n",
            "[Epoch  51/100] [Batch 500/582] [D loss: 1.00487] [G loss: 0.50332]\n",
            "[Epoch  51/100] [Time: 27.31] [D loss: 1.00148] [G loss: 0.50410]\n",
            "[Epoch  52/100] [Batch  18/582] [D loss: 1.01073] [G loss: 0.50587]\n",
            "[Epoch  52/100] [Batch 118/582] [D loss: 1.00543] [G loss: 0.50501]\n",
            "[Epoch  52/100] [Batch 218/582] [D loss: 1.01135] [G loss: 0.50700]\n",
            "[Epoch  52/100] [Batch 318/582] [D loss: 1.00589] [G loss: 0.50318]\n",
            "[Epoch  52/100] [Batch 418/582] [D loss: 0.99986] [G loss: 0.50615]\n",
            "[Epoch  52/100] [Batch 518/582] [D loss: 1.01117] [G loss: 0.50769]\n",
            "[Epoch  52/100] [Time: 27.06] [D loss: 1.00235] [G loss: 0.50833]\n",
            "[Epoch  53/100] [Batch  36/582] [D loss: 0.99521] [G loss: 0.50111]\n",
            "[Epoch  53/100] [Batch 136/582] [D loss: 0.99483] [G loss: 0.50195]\n",
            "[Epoch  53/100] [Batch 236/582] [D loss: 0.99291] [G loss: 0.50124]\n",
            "[Epoch  53/100] [Batch 336/582] [D loss: 0.99391] [G loss: 0.50304]\n",
            "[Epoch  53/100] [Batch 436/582] [D loss: 0.99976] [G loss: 0.50218]\n",
            "[Epoch  53/100] [Batch 536/582] [D loss: 0.99467] [G loss: 0.50094]\n",
            "[Epoch  53/100] [Time: 25.57] [D loss: 1.01324] [G loss: 0.50802]\n",
            "[Epoch  54/100] [Batch  54/582] [D loss: 0.99715] [G loss: 0.49857]\n",
            "[Epoch  54/100] [Batch 154/582] [D loss: 0.99836] [G loss: 0.50119]\n",
            "[Epoch  54/100] [Batch 254/582] [D loss: 1.00110] [G loss: 0.50540]\n",
            "[Epoch  54/100] [Batch 354/582] [D loss: 1.00177] [G loss: 0.50469]\n",
            "[Epoch  54/100] [Batch 454/582] [D loss: 1.00509] [G loss: 0.50484]\n",
            "[Epoch  54/100] [Batch 554/582] [D loss: 0.99923] [G loss: 0.50309]\n",
            "[Epoch  54/100] [Time: 27.33] [D loss: 1.00033] [G loss: 0.50284]\n",
            "[Epoch  55/100] [Batch  72/582] [D loss: 1.00458] [G loss: 0.50440]\n",
            "[Epoch  55/100] [Batch 172/582] [D loss: 1.00362] [G loss: 0.50665]\n",
            "[Epoch  55/100] [Batch 272/582] [D loss: 0.99647] [G loss: 0.50227]\n",
            "[Epoch  55/100] [Batch 372/582] [D loss: 0.99903] [G loss: 0.50351]\n",
            "[Epoch  55/100] [Batch 472/582] [D loss: 0.99427] [G loss: 0.50551]\n",
            "[Epoch  55/100] [Batch 572/582] [D loss: 1.00476] [G loss: 0.50216]\n",
            "[Epoch  55/100] [Time: 27.35] [D loss: 1.00070] [G loss: 0.50315]\n",
            "[Epoch  56/100] [Batch  90/582] [D loss: 0.99507] [G loss: 0.50212]\n",
            "[Epoch  56/100] [Batch 190/582] [D loss: 0.99354] [G loss: 0.50133]\n",
            "[Epoch  56/100] [Batch 290/582] [D loss: 1.00135] [G loss: 0.50326]\n",
            "[Epoch  56/100] [Batch 390/582] [D loss: 1.00761] [G loss: 0.50723]\n",
            "[Epoch  56/100] [Batch 490/582] [D loss: 0.99790] [G loss: 0.50616]\n",
            "[Epoch  56/100] [Time: 27.46] [D loss: 1.00084] [G loss: 0.50414]\n",
            "[Epoch  57/100] [Batch   8/582] [D loss: 0.99682] [G loss: 0.50443]\n",
            "[Epoch  57/100] [Batch 108/582] [D loss: 1.00107] [G loss: 0.50303]\n",
            "[Epoch  57/100] [Batch 208/582] [D loss: 1.00071] [G loss: 0.50924]\n",
            "[Epoch  57/100] [Batch 308/582] [D loss: 0.99685] [G loss: 0.50249]\n",
            "[Epoch  57/100] [Batch 408/582] [D loss: 1.00593] [G loss: 0.50503]\n",
            "[Epoch  57/100] [Batch 508/582] [D loss: 1.01590] [G loss: 0.50854]\n",
            "[Epoch  57/100] [Time: 27.36] [D loss: 0.99863] [G loss: 0.50076]\n",
            "[Epoch  58/100] [Batch  26/582] [D loss: 1.00021] [G loss: 0.50746]\n",
            "[Epoch  58/100] [Batch 126/582] [D loss: 1.00368] [G loss: 0.50281]\n",
            "[Epoch  58/100] [Batch 226/582] [D loss: 1.00103] [G loss: 0.50809]\n",
            "[Epoch  58/100] [Batch 326/582] [D loss: 1.00595] [G loss: 0.50426]\n",
            "[Epoch  58/100] [Batch 426/582] [D loss: 1.01375] [G loss: 0.50776]\n",
            "[Epoch  58/100] [Batch 526/582] [D loss: 0.99799] [G loss: 0.50200]\n",
            "[Epoch  58/100] [Time: 27.37] [D loss: 1.00771] [G loss: 0.50899]\n",
            "[Epoch  59/100] [Batch  44/582] [D loss: 0.99005] [G loss: 0.49784]\n",
            "[Epoch  59/100] [Batch 144/582] [D loss: 1.01511] [G loss: 0.50836]\n",
            "[Epoch  59/100] [Batch 244/582] [D loss: 1.00175] [G loss: 0.50439]\n",
            "[Epoch  59/100] [Batch 344/582] [D loss: 1.00210] [G loss: 0.50282]\n",
            "[Epoch  59/100] [Batch 444/582] [D loss: 0.99779] [G loss: 0.50751]\n",
            "[Epoch  59/100] [Batch 544/582] [D loss: 1.00060] [G loss: 0.50595]\n",
            "[Epoch  59/100] [Time: 27.41] [D loss: 1.00076] [G loss: 0.50906]\n",
            "[Epoch  60/100] [Batch  62/582] [D loss: 1.00128] [G loss: 0.50428]\n",
            "[Epoch  60/100] [Batch 162/582] [D loss: 1.01082] [G loss: 0.50581]\n",
            "[Epoch  60/100] [Batch 262/582] [D loss: 1.01427] [G loss: 0.50785]\n",
            "[Epoch  60/100] [Batch 362/582] [D loss: 1.00024] [G loss: 0.50206]\n",
            "[Epoch  60/100] [Batch 462/582] [D loss: 1.01353] [G loss: 0.50714]\n",
            "[Epoch  60/100] [Batch 562/582] [D loss: 1.00199] [G loss: 0.50718]\n",
            "[Epoch  60/100] [Time: 27.25] [D loss: 1.00349] [G loss: 0.50654]\n",
            "[Epoch  61/100] [Batch  80/582] [D loss: 0.99954] [G loss: 0.49859]\n",
            "[Epoch  61/100] [Batch 180/582] [D loss: 1.00277] [G loss: 0.50294]\n",
            "[Epoch  61/100] [Batch 280/582] [D loss: 1.00440] [G loss: 0.50397]\n",
            "[Epoch  61/100] [Batch 380/582] [D loss: 1.00901] [G loss: 0.50441]\n",
            "[Epoch  61/100] [Batch 480/582] [D loss: 1.00722] [G loss: 0.50653]\n",
            "[Epoch  61/100] [Batch 580/582] [D loss: 0.99568] [G loss: 0.50363]\n",
            "[Epoch  61/100] [Time: 26.38] [D loss: 1.00096] [G loss: 0.50674]\n",
            "[Epoch  62/100] [Batch  98/582] [D loss: 0.99927] [G loss: 0.50216]\n",
            "[Epoch  62/100] [Batch 198/582] [D loss: 1.00182] [G loss: 0.50281]\n",
            "[Epoch  62/100] [Batch 298/582] [D loss: 1.01643] [G loss: 0.50852]\n",
            "[Epoch  62/100] [Batch 398/582] [D loss: 0.99905] [G loss: 0.50255]\n",
            "[Epoch  62/100] [Batch 498/582] [D loss: 1.00305] [G loss: 0.49897]\n",
            "[Epoch  62/100] [Time: 27.57] [D loss: 0.99405] [G loss: 0.50465]\n",
            "[Epoch  63/100] [Batch  16/582] [D loss: 1.00551] [G loss: 0.50305]\n",
            "[Epoch  63/100] [Batch 116/582] [D loss: 1.01515] [G loss: 0.50975]\n",
            "[Epoch  63/100] [Batch 216/582] [D loss: 0.99859] [G loss: 0.50607]\n",
            "[Epoch  63/100] [Batch 316/582] [D loss: 1.00911] [G loss: 0.50504]\n",
            "[Epoch  63/100] [Batch 416/582] [D loss: 0.99381] [G loss: 0.50202]\n",
            "[Epoch  63/100] [Batch 516/582] [D loss: 0.99796] [G loss: 0.50819]\n",
            "[Epoch  63/100] [Time: 27.41] [D loss: 1.00596] [G loss: 0.50524]\n",
            "[Epoch  64/100] [Batch  34/582] [D loss: 1.00559] [G loss: 0.50662]\n",
            "[Epoch  64/100] [Batch 134/582] [D loss: 1.00362] [G loss: 0.50720]\n",
            "[Epoch  64/100] [Batch 234/582] [D loss: 0.99793] [G loss: 0.49620]\n",
            "[Epoch  64/100] [Batch 334/582] [D loss: 0.99700] [G loss: 0.50748]\n",
            "[Epoch  64/100] [Batch 434/582] [D loss: 0.99887] [G loss: 0.50738]\n",
            "[Epoch  64/100] [Batch 534/582] [D loss: 0.99898] [G loss: 0.50515]\n",
            "[Epoch  64/100] [Time: 27.32] [D loss: 1.00200] [G loss: 0.50632]\n",
            "[Epoch  65/100] [Batch  52/582] [D loss: 0.99963] [G loss: 0.50487]\n",
            "[Epoch  65/100] [Batch 152/582] [D loss: 0.99491] [G loss: 0.50381]\n",
            "[Epoch  65/100] [Batch 252/582] [D loss: 1.00076] [G loss: 0.50760]\n",
            "[Epoch  65/100] [Batch 352/582] [D loss: 0.99753] [G loss: 0.50897]\n",
            "[Epoch  65/100] [Batch 452/582] [D loss: 0.99832] [G loss: 0.50903]\n",
            "[Epoch  65/100] [Batch 552/582] [D loss: 0.99892] [G loss: 0.50456]\n",
            "[Epoch  65/100] [Time: 27.28] [D loss: 0.99821] [G loss: 0.50789]\n",
            "[Epoch  66/100] [Batch  70/582] [D loss: 0.99627] [G loss: 0.50510]\n",
            "[Epoch  66/100] [Batch 170/582] [D loss: 0.99905] [G loss: 0.50139]\n",
            "[Epoch  66/100] [Batch 270/582] [D loss: 1.00336] [G loss: 0.50353]\n",
            "[Epoch  66/100] [Batch 370/582] [D loss: 1.00024] [G loss: 0.50858]\n",
            "[Epoch  66/100] [Batch 470/582] [D loss: 0.99796] [G loss: 0.50785]\n",
            "[Epoch  66/100] [Batch 570/582] [D loss: 0.99916] [G loss: 0.50756]\n",
            "[Epoch  66/100] [Time: 20.82] [D loss: 0.99638] [G loss: 0.50663]\n",
            "[Epoch  67/100] [Batch  88/582] [D loss: 0.99689] [G loss: 0.50784]\n",
            "[Epoch  67/100] [Batch 188/582] [D loss: 0.99741] [G loss: 0.50447]\n",
            "[Epoch  67/100] [Batch 288/582] [D loss: 1.00227] [G loss: 0.50327]\n",
            "[Epoch  67/100] [Batch 388/582] [D loss: 1.00128] [G loss: 0.50336]\n",
            "[Epoch  67/100] [Batch 488/582] [D loss: 0.99522] [G loss: 0.50919]\n",
            "[Epoch  67/100] [Time: 27.51] [D loss: 0.99855] [G loss: 0.50604]\n",
            "[Epoch  68/100] [Batch   6/582] [D loss: 0.99947] [G loss: 0.50484]\n",
            "[Epoch  68/100] [Batch 106/582] [D loss: 1.00089] [G loss: 0.50601]\n",
            "[Epoch  68/100] [Batch 206/582] [D loss: 0.99751] [G loss: 0.50402]\n",
            "[Epoch  68/100] [Batch 306/582] [D loss: 0.99885] [G loss: 0.50793]\n",
            "[Epoch  68/100] [Batch 406/582] [D loss: 0.99660] [G loss: 0.50745]\n",
            "[Epoch  68/100] [Batch 506/582] [D loss: 1.00042] [G loss: 0.50718]\n",
            "[Epoch  68/100] [Time: 27.35] [D loss: 0.99981] [G loss: 0.50860]\n",
            "[Epoch  69/100] [Batch  24/582] [D loss: 0.99658] [G loss: 0.50783]\n",
            "[Epoch  69/100] [Batch 124/582] [D loss: 0.99509] [G loss: 0.50394]\n",
            "[Epoch  69/100] [Batch 224/582] [D loss: 1.00050] [G loss: 0.50604]\n",
            "[Epoch  69/100] [Batch 324/582] [D loss: 0.99914] [G loss: 0.50746]\n",
            "[Epoch  69/100] [Batch 424/582] [D loss: 0.99367] [G loss: 0.50507]\n",
            "[Epoch  69/100] [Batch 524/582] [D loss: 1.00040] [G loss: 0.50558]\n",
            "[Epoch  69/100] [Time: 27.27] [D loss: 1.00310] [G loss: 0.50807]\n",
            "[Epoch  70/100] [Batch  42/582] [D loss: 0.99877] [G loss: 0.50175]\n",
            "[Epoch  70/100] [Batch 142/582] [D loss: 0.99904] [G loss: 0.50889]\n",
            "[Epoch  70/100] [Batch 242/582] [D loss: 0.99800] [G loss: 0.50862]\n",
            "[Epoch  70/100] [Batch 342/582] [D loss: 0.99616] [G loss: 0.50621]\n",
            "[Epoch  70/100] [Batch 442/582] [D loss: 0.99805] [G loss: 0.50432]\n",
            "[Epoch  70/100] [Batch 542/582] [D loss: 0.99913] [G loss: 0.50707]\n",
            "[Epoch  70/100] [Time: 27.37] [D loss: 1.01620] [G loss: 0.50841]\n",
            "[Epoch  71/100] [Batch  60/582] [D loss: 0.99870] [G loss: 0.50672]\n",
            "[Epoch  71/100] [Batch 160/582] [D loss: 0.99625] [G loss: 0.50721]\n",
            "[Epoch  71/100] [Batch 260/582] [D loss: 1.00001] [G loss: 0.50419]\n",
            "[Epoch  71/100] [Batch 360/582] [D loss: 0.99871] [G loss: 0.50506]\n",
            "[Epoch  71/100] [Batch 460/582] [D loss: 0.99339] [G loss: 0.50654]\n",
            "[Epoch  71/100] [Batch 560/582] [D loss: 1.00793] [G loss: 0.50623]\n",
            "[Epoch  71/100] [Time: 27.33] [D loss: 0.99850] [G loss: 0.50326]\n",
            "[Epoch  72/100] [Batch  78/582] [D loss: 1.00802] [G loss: 0.50397]\n",
            "[Epoch  72/100] [Batch 178/582] [D loss: 0.99742] [G loss: 0.50547]\n",
            "[Epoch  72/100] [Batch 278/582] [D loss: 1.00012] [G loss: 0.50801]\n",
            "[Epoch  72/100] [Batch 378/582] [D loss: 1.00681] [G loss: 0.50637]\n",
            "[Epoch  72/100] [Batch 478/582] [D loss: 0.98994] [G loss: 0.50371]\n",
            "[Epoch  72/100] [Batch 578/582] [D loss: 1.00245] [G loss: 0.50461]\n",
            "[Epoch  72/100] [Time: 27.29] [D loss: 1.00025] [G loss: 0.50915]\n",
            "[Epoch  73/100] [Batch  96/582] [D loss: 0.99955] [G loss: 0.50139]\n",
            "[Epoch  73/100] [Batch 196/582] [D loss: 1.00749] [G loss: 0.50376]\n",
            "[Epoch  73/100] [Batch 296/582] [D loss: 1.00852] [G loss: 0.50613]\n",
            "[Epoch  73/100] [Batch 396/582] [D loss: 1.01348] [G loss: 0.50875]\n",
            "[Epoch  73/100] [Batch 496/582] [D loss: 0.99934] [G loss: 0.50385]\n",
            "[Epoch  73/100] [Time: 27.46] [D loss: 0.99502] [G loss: 0.50165]\n",
            "[Epoch  74/100] [Batch  14/582] [D loss: 0.99349] [G loss: 0.50201]\n",
            "[Epoch  74/100] [Batch 114/582] [D loss: 0.99506] [G loss: 0.50712]\n",
            "[Epoch  74/100] [Batch 214/582] [D loss: 1.00330] [G loss: 0.50446]\n",
            "[Epoch  74/100] [Batch 314/582] [D loss: 1.00907] [G loss: 0.50488]\n",
            "[Epoch  74/100] [Batch 414/582] [D loss: 0.99624] [G loss: 0.50625]\n",
            "[Epoch  74/100] [Batch 514/582] [D loss: 0.99665] [G loss: 0.50592]\n",
            "[Epoch  74/100] [Time: 27.33] [D loss: 0.99784] [G loss: 0.50893]\n",
            "[Epoch  75/100] [Batch  32/582] [D loss: 1.01575] [G loss: 0.50961]\n",
            "[Epoch  75/100] [Batch 132/582] [D loss: 0.99818] [G loss: 0.50760]\n",
            "[Epoch  75/100] [Batch 232/582] [D loss: 1.00553] [G loss: 0.50467]\n",
            "[Epoch  75/100] [Batch 332/582] [D loss: 1.00203] [G loss: 0.50567]\n",
            "[Epoch  75/100] [Batch 432/582] [D loss: 1.00089] [G loss: 0.50448]\n",
            "[Epoch  75/100] [Batch 532/582] [D loss: 0.99780] [G loss: 0.50349]\n",
            "[Epoch  75/100] [Time: 27.52] [D loss: 1.00789] [G loss: 0.50357]\n",
            "[Epoch  76/100] [Batch  50/582] [D loss: 1.00832] [G loss: 0.50558]\n",
            "[Epoch  76/100] [Batch 150/582] [D loss: 1.00408] [G loss: 0.50324]\n",
            "[Epoch  76/100] [Batch 250/582] [D loss: 0.98997] [G loss: 0.49869]\n",
            "[Epoch  76/100] [Batch 350/582] [D loss: 0.99996] [G loss: 0.50646]\n",
            "[Epoch  76/100] [Batch 450/582] [D loss: 1.00048] [G loss: 0.50876]\n",
            "[Epoch  76/100] [Batch 550/582] [D loss: 1.00233] [G loss: 0.50613]\n",
            "[Epoch  76/100] [Time: 27.37] [D loss: 1.00576] [G loss: 0.50545]\n",
            "[Epoch  77/100] [Batch  68/582] [D loss: 1.01062] [G loss: 0.50754]\n",
            "[Epoch  77/100] [Batch 168/582] [D loss: 0.99754] [G loss: 0.50888]\n",
            "[Epoch  77/100] [Batch 268/582] [D loss: 0.99717] [G loss: 0.50682]\n",
            "[Epoch  77/100] [Batch 368/582] [D loss: 1.00036] [G loss: 0.50277]\n",
            "[Epoch  77/100] [Batch 468/582] [D loss: 1.01187] [G loss: 0.50700]\n",
            "[Epoch  77/100] [Batch 568/582] [D loss: 0.99637] [G loss: 0.50579]\n",
            "[Epoch  77/100] [Time: 27.16] [D loss: 0.99287] [G loss: 0.50821]\n",
            "[Epoch  78/100] [Batch  86/582] [D loss: 1.00202] [G loss: 0.50736]\n",
            "[Epoch  78/100] [Batch 186/582] [D loss: 0.99884] [G loss: 0.50577]\n",
            "[Epoch  78/100] [Batch 286/582] [D loss: 0.99062] [G loss: 0.50833]\n",
            "[Epoch  78/100] [Batch 386/582] [D loss: 0.99515] [G loss: 0.49479]\n",
            "[Epoch  78/100] [Batch 486/582] [D loss: 0.99607] [G loss: 0.50359]\n",
            "[Epoch  78/100] [Time: 27.53] [D loss: 0.99791] [G loss: 0.50881]\n",
            "[Epoch  79/100] [Batch   4/582] [D loss: 0.99768] [G loss: 0.50561]\n",
            "[Epoch  79/100] [Batch 104/582] [D loss: 0.99886] [G loss: 0.50189]\n",
            "[Epoch  79/100] [Batch 204/582] [D loss: 1.00774] [G loss: 0.50698]\n",
            "[Epoch  79/100] [Batch 304/582] [D loss: 1.01300] [G loss: 0.50864]\n",
            "[Epoch  79/100] [Batch 404/582] [D loss: 1.01103] [G loss: 0.50835]\n",
            "[Epoch  79/100] [Batch 504/582] [D loss: 0.99741] [G loss: 0.50792]\n",
            "[Epoch  79/100] [Time: 27.36] [D loss: 1.00517] [G loss: 0.50924]\n",
            "[Epoch  80/100] [Batch  22/582] [D loss: 0.99958] [G loss: 0.50844]\n",
            "[Epoch  80/100] [Batch 122/582] [D loss: 1.00059] [G loss: 0.50752]\n",
            "[Epoch  80/100] [Batch 222/582] [D loss: 0.99823] [G loss: 0.50443]\n",
            "[Epoch  80/100] [Batch 322/582] [D loss: 1.00210] [G loss: 0.50354]\n",
            "[Epoch  80/100] [Batch 422/582] [D loss: 1.01408] [G loss: 0.50889]\n",
            "[Epoch  80/100] [Batch 522/582] [D loss: 0.99601] [G loss: 0.50646]\n",
            "[Epoch  80/100] [Time: 26.86] [D loss: 0.99941] [G loss: 0.50535]\n",
            "[Epoch  81/100] [Batch  40/582] [D loss: 0.99980] [G loss: 0.50556]\n",
            "[Epoch  81/100] [Batch 140/582] [D loss: 0.99860] [G loss: 0.50774]\n",
            "[Epoch  81/100] [Batch 240/582] [D loss: 0.99626] [G loss: 0.50337]\n",
            "[Epoch  81/100] [Batch 340/582] [D loss: 0.99805] [G loss: 0.50673]\n",
            "[Epoch  81/100] [Batch 440/582] [D loss: 0.99901] [G loss: 0.50421]\n",
            "[Epoch  81/100] [Batch 540/582] [D loss: 0.99908] [G loss: 0.50831]\n",
            "[Epoch  81/100] [Time: 15.94] [D loss: 0.99924] [G loss: 0.50889]\n",
            "[Epoch  82/100] [Batch  58/582] [D loss: 1.00053] [G loss: 0.50492]\n",
            "[Epoch  82/100] [Batch 158/582] [D loss: 0.99934] [G loss: 0.50616]\n",
            "[Epoch  82/100] [Batch 258/582] [D loss: 0.99494] [G loss: 0.50569]\n",
            "[Epoch  82/100] [Batch 358/582] [D loss: 0.99418] [G loss: 0.50677]\n",
            "[Epoch  82/100] [Batch 458/582] [D loss: 0.99608] [G loss: 0.50545]\n",
            "[Epoch  82/100] [Batch 558/582] [D loss: 1.00641] [G loss: 0.50664]\n",
            "[Epoch  82/100] [Time: 27.24] [D loss: 0.99776] [G loss: 0.50875]\n",
            "[Epoch  83/100] [Batch  76/582] [D loss: 0.99884] [G loss: 0.50973]\n",
            "[Epoch  83/100] [Batch 176/582] [D loss: 0.99864] [G loss: 0.50897]\n",
            "[Epoch  83/100] [Batch 276/582] [D loss: 0.99722] [G loss: 0.50590]\n",
            "[Epoch  83/100] [Batch 376/582] [D loss: 1.00278] [G loss: 0.50323]\n",
            "[Epoch  83/100] [Batch 476/582] [D loss: 0.99478] [G loss: 0.50719]\n",
            "[Epoch  83/100] [Batch 576/582] [D loss: 0.99967] [G loss: 0.50746]\n",
            "[Epoch  83/100] [Time: 27.35] [D loss: 1.01341] [G loss: 0.50761]\n",
            "[Epoch  84/100] [Batch  94/582] [D loss: 0.99950] [G loss: 0.50700]\n",
            "[Epoch  84/100] [Batch 194/582] [D loss: 0.99764] [G loss: 0.50918]\n",
            "[Epoch  84/100] [Batch 294/582] [D loss: 0.99773] [G loss: 0.50966]\n",
            "[Epoch  84/100] [Batch 394/582] [D loss: 1.00034] [G loss: 0.50492]\n",
            "[Epoch  84/100] [Batch 494/582] [D loss: 0.99925] [G loss: 0.50510]\n",
            "[Epoch  84/100] [Time: 27.42] [D loss: 0.99774] [G loss: 0.50048]\n",
            "[Epoch  85/100] [Batch  12/582] [D loss: 0.99922] [G loss: 0.50833]\n",
            "[Epoch  85/100] [Batch 112/582] [D loss: 0.99785] [G loss: 0.50193]\n",
            "[Epoch  85/100] [Batch 212/582] [D loss: 0.99842] [G loss: 0.50261]\n",
            "[Epoch  85/100] [Batch 312/582] [D loss: 1.00241] [G loss: 0.50566]\n",
            "[Epoch  85/100] [Batch 412/582] [D loss: 0.99724] [G loss: 0.50712]\n",
            "[Epoch  85/100] [Batch 512/582] [D loss: 0.99807] [G loss: 0.50646]\n",
            "[Epoch  85/100] [Time: 27.27] [D loss: 1.00007] [G loss: 0.51044]\n",
            "[Epoch  86/100] [Batch  30/582] [D loss: 0.99601] [G loss: 0.50370]\n",
            "[Epoch  86/100] [Batch 130/582] [D loss: 0.99921] [G loss: 0.50456]\n",
            "[Epoch  86/100] [Batch 230/582] [D loss: 0.99847] [G loss: 0.50613]\n",
            "[Epoch  86/100] [Batch 330/582] [D loss: 0.99586] [G loss: 0.50669]\n",
            "[Epoch  86/100] [Batch 430/582] [D loss: 0.99795] [G loss: 0.50182]\n",
            "[Epoch  86/100] [Batch 530/582] [D loss: 0.99828] [G loss: 0.50863]\n",
            "[Epoch  86/100] [Time: 24.55] [D loss: 0.99913] [G loss: 0.50805]\n",
            "[Epoch  87/100] [Batch  48/582] [D loss: 0.99743] [G loss: 0.50700]\n",
            "[Epoch  87/100] [Batch 148/582] [D loss: 0.99677] [G loss: 0.50674]\n",
            "[Epoch  87/100] [Batch 248/582] [D loss: 0.99574] [G loss: 0.50666]\n",
            "[Epoch  87/100] [Batch 348/582] [D loss: 0.99628] [G loss: 0.50239]\n",
            "[Epoch  87/100] [Batch 448/582] [D loss: 1.00655] [G loss: 0.50585]\n",
            "[Epoch  87/100] [Batch 548/582] [D loss: 0.99867] [G loss: 0.50783]\n",
            "[Epoch  87/100] [Time: 26.70] [D loss: 0.99915] [G loss: 0.50960]\n",
            "[Epoch  88/100] [Batch  66/582] [D loss: 0.99672] [G loss: 0.50476]\n",
            "[Epoch  88/100] [Batch 166/582] [D loss: 1.00231] [G loss: 0.50450]\n",
            "[Epoch  88/100] [Batch 266/582] [D loss: 0.99411] [G loss: 0.50495]\n",
            "[Epoch  88/100] [Batch 366/582] [D loss: 0.99968] [G loss: 0.50967]\n",
            "[Epoch  88/100] [Batch 466/582] [D loss: 1.01470] [G loss: 0.50742]\n",
            "[Epoch  88/100] [Batch 566/582] [D loss: 1.00307] [G loss: 0.50382]\n",
            "[Epoch  88/100] [Time: 27.44] [D loss: 0.99844] [G loss: 0.50821]\n",
            "[Epoch  89/100] [Batch  84/582] [D loss: 0.99642] [G loss: 0.50252]\n",
            "[Epoch  89/100] [Batch 184/582] [D loss: 0.99675] [G loss: 0.50319]\n",
            "[Epoch  89/100] [Batch 284/582] [D loss: 0.99325] [G loss: 0.50627]\n",
            "[Epoch  89/100] [Batch 384/582] [D loss: 1.00001] [G loss: 0.50665]\n",
            "[Epoch  89/100] [Batch 484/582] [D loss: 1.00066] [G loss: 0.50743]\n",
            "[Epoch  89/100] [Time: 27.42] [D loss: 0.99253] [G loss: 0.50609]\n",
            "[Epoch  90/100] [Batch   2/582] [D loss: 0.99423] [G loss: 0.49907]\n",
            "[Epoch  90/100] [Batch 102/582] [D loss: 0.99427] [G loss: 0.50634]\n",
            "[Epoch  90/100] [Batch 202/582] [D loss: 1.00428] [G loss: 0.50286]\n",
            "[Epoch  90/100] [Batch 302/582] [D loss: 1.00623] [G loss: 0.50563]\n",
            "[Epoch  90/100] [Batch 402/582] [D loss: 1.00832] [G loss: 0.50559]\n",
            "[Epoch  90/100] [Batch 502/582] [D loss: 0.99970] [G loss: 0.50257]\n",
            "[Epoch  90/100] [Time: 27.41] [D loss: 0.99626] [G loss: 0.50152]\n",
            "[Epoch  91/100] [Batch  20/582] [D loss: 1.00795] [G loss: 0.50795]\n",
            "[Epoch  91/100] [Batch 120/582] [D loss: 1.00022] [G loss: 0.50693]\n",
            "[Epoch  91/100] [Batch 220/582] [D loss: 1.00232] [G loss: 0.50201]\n",
            "[Epoch  91/100] [Batch 320/582] [D loss: 1.00387] [G loss: 0.50456]\n",
            "[Epoch  91/100] [Batch 420/582] [D loss: 1.01132] [G loss: 0.50791]\n",
            "[Epoch  91/100] [Batch 520/582] [D loss: 0.99423] [G loss: 0.50408]\n",
            "[Epoch  91/100] [Time: 27.60] [D loss: 0.99581] [G loss: 0.50133]\n",
            "[Epoch  92/100] [Batch  38/582] [D loss: 0.99978] [G loss: 0.50836]\n",
            "[Epoch  92/100] [Batch 138/582] [D loss: 1.00539] [G loss: 0.50270]\n",
            "[Epoch  92/100] [Batch 238/582] [D loss: 1.00455] [G loss: 0.50237]\n",
            "[Epoch  92/100] [Batch 338/582] [D loss: 1.00485] [G loss: 0.50383]\n",
            "[Epoch  92/100] [Batch 438/582] [D loss: 1.00045] [G loss: 0.50381]\n",
            "[Epoch  92/100] [Batch 538/582] [D loss: 0.99658] [G loss: 0.50014]\n",
            "[Epoch  92/100] [Time: 27.32] [D loss: 0.99566] [G loss: 0.50572]\n",
            "[Epoch  93/100] [Batch  56/582] [D loss: 1.01134] [G loss: 0.50649]\n",
            "[Epoch  93/100] [Batch 156/582] [D loss: 1.01580] [G loss: 0.50803]\n",
            "[Epoch  93/100] [Batch 256/582] [D loss: 1.00184] [G loss: 0.50356]\n",
            "[Epoch  93/100] [Batch 356/582] [D loss: 1.01570] [G loss: 0.50897]\n",
            "[Epoch  93/100] [Batch 456/582] [D loss: 0.99951] [G loss: 0.50119]\n",
            "[Epoch  93/100] [Batch 556/582] [D loss: 1.00014] [G loss: 0.50726]\n",
            "[Epoch  93/100] [Time: 27.24] [D loss: 1.00163] [G loss: 0.50806]\n",
            "[Epoch  94/100] [Batch  74/582] [D loss: 0.99591] [G loss: 0.49871]\n",
            "[Epoch  94/100] [Batch 174/582] [D loss: 0.99900] [G loss: 0.49769]\n",
            "[Epoch  94/100] [Batch 274/582] [D loss: 0.99144] [G loss: 0.50105]\n",
            "[Epoch  94/100] [Batch 374/582] [D loss: 0.99988] [G loss: 0.50397]\n",
            "[Epoch  94/100] [Batch 474/582] [D loss: 0.99673] [G loss: 0.50196]\n",
            "[Epoch  94/100] [Batch 574/582] [D loss: 0.99704] [G loss: 0.50703]\n",
            "[Epoch  94/100] [Time: 27.33] [D loss: 0.99786] [G loss: 0.50546]\n",
            "[Epoch  95/100] [Batch  92/582] [D loss: 0.99536] [G loss: 0.50763]\n",
            "[Epoch  95/100] [Batch 192/582] [D loss: 1.01388] [G loss: 0.50827]\n",
            "[Epoch  95/100] [Batch 292/582] [D loss: 1.00324] [G loss: 0.50580]\n",
            "[Epoch  95/100] [Batch 392/582] [D loss: 0.99908] [G loss: 0.50431]\n",
            "[Epoch  95/100] [Batch 492/582] [D loss: 0.99979] [G loss: 0.49989]\n",
            "[Epoch  95/100] [Time: 27.41] [D loss: 1.00722] [G loss: 0.50515]\n",
            "[Epoch  96/100] [Batch  10/582] [D loss: 1.01002] [G loss: 0.50631]\n",
            "[Epoch  96/100] [Batch 110/582] [D loss: 1.00614] [G loss: 0.50614]\n",
            "[Epoch  96/100] [Batch 210/582] [D loss: 1.01373] [G loss: 0.50901]\n",
            "[Epoch  96/100] [Batch 310/582] [D loss: 1.00154] [G loss: 0.50295]\n",
            "[Epoch  96/100] [Batch 410/582] [D loss: 0.99787] [G loss: 0.50343]\n",
            "[Epoch  96/100] [Batch 510/582] [D loss: 0.99998] [G loss: 0.50450]\n",
            "[Epoch  96/100] [Time: 27.54] [D loss: 1.00750] [G loss: 0.50633]\n",
            "[Epoch  97/100] [Batch  28/582] [D loss: 1.01387] [G loss: 0.50772]\n",
            "[Epoch  97/100] [Batch 128/582] [D loss: 0.99817] [G loss: 0.50723]\n",
            "[Epoch  97/100] [Batch 228/582] [D loss: 1.00584] [G loss: 0.50398]\n",
            "[Epoch  97/100] [Batch 328/582] [D loss: 0.99530] [G loss: 0.49880]\n",
            "[Epoch  97/100] [Batch 428/582] [D loss: 1.01259] [G loss: 0.50718]\n",
            "[Epoch  97/100] [Batch 528/582] [D loss: 1.00396] [G loss: 0.50664]\n",
            "[Epoch  97/100] [Time: 27.54] [D loss: 1.01104] [G loss: 0.50689]\n",
            "[Epoch  98/100] [Batch  46/582] [D loss: 0.99936] [G loss: 0.50319]\n",
            "[Epoch  98/100] [Batch 146/582] [D loss: 1.00068] [G loss: 0.50361]\n",
            "[Epoch  98/100] [Batch 246/582] [D loss: 1.00338] [G loss: 0.50693]\n",
            "[Epoch  98/100] [Batch 346/582] [D loss: 1.00562] [G loss: 0.50398]\n",
            "[Epoch  98/100] [Batch 446/582] [D loss: 0.99853] [G loss: 0.49880]\n",
            "[Epoch  98/100] [Batch 546/582] [D loss: 0.99772] [G loss: 0.50250]\n",
            "[Epoch  98/100] [Time: 27.17] [D loss: 0.99760] [G loss: 0.50152]\n",
            "[Epoch  99/100] [Batch  64/582] [D loss: 1.00850] [G loss: 0.50640]\n",
            "[Epoch  99/100] [Batch 164/582] [D loss: 0.99807] [G loss: 0.49837]\n",
            "[Epoch  99/100] [Batch 264/582] [D loss: 0.99735] [G loss: 0.50211]\n",
            "[Epoch  99/100] [Batch 364/582] [D loss: 1.00195] [G loss: 0.50236]\n",
            "[Epoch  99/100] [Batch 464/582] [D loss: 1.00568] [G loss: 0.50580]\n",
            "[Epoch  99/100] [Batch 564/582] [D loss: 1.00340] [G loss: 0.50403]\n",
            "[Epoch  99/100] [Time: 27.50] [D loss: 0.99345] [G loss: 0.50292]\n",
            "[Epoch 100/100] [Batch  82/582] [D loss: 1.00947] [G loss: 0.50568]\n",
            "[Epoch 100/100] [Batch 182/582] [D loss: 0.99830] [G loss: 0.49987]\n",
            "[Epoch 100/100] [Batch 282/582] [D loss: 1.01050] [G loss: 0.50540]\n",
            "[Epoch 100/100] [Batch 382/582] [D loss: 0.99650] [G loss: 0.50196]\n",
            "[Epoch 100/100] [Batch 482/582] [D loss: 0.99981] [G loss: 0.50121]\n",
            "[Epoch 100/100] [Batch   0/582] [D loss: 1.01215] [G loss: 0.50782]\n",
            "[Epoch 100/100] [Time: 27.58] [D loss: 1.01215] [G loss: 0.50782]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEcQ_acg1xqu",
        "outputId": "4bb069b3-825c-4168-d5bf-91d881d42bb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "autoencoder.eval()\n",
        "generator.eval()\n",
        "discriminator.eval()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (model): Sequential(\n",
              "    (0): Linear(in_features=2142, out_features=1071, bias=True)\n",
              "    (1): Conv1d(1, 16, kernel_size=(8,), stride=(4,), padding=(1,))\n",
              "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (3): Conv1d(16, 32, kernel_size=(8,), stride=(4,), padding=(1,))\n",
              "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (6): Conv1d(32, 64, kernel_size=(8,), stride=(4,), padding=(1,))\n",
              "    (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (9): Conv1d(64, 128, kernel_size=(8,), stride=(4,), padding=(1,))\n",
              "    (10): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (12): Conv1d(128, 1, kernel_size=(3,), stride=(1,))\n",
              "    (13): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuCmebS-zPp5"
      },
      "source": [
        "num_fake_batches = 80\n",
        "fake_data = torch.zeros((0, feature_size), device='cpu')\n",
        "for _ in range(num_fake_batches):\n",
        "  z = torch.randn(opt.batch_size, 128, device=device)\n",
        "  generated_batch = generator(z)\n",
        "  fake_batch = torch.squeeze(autoencoder.decode(generator(z).unsqueeze(dim=2)))\n",
        "  fake_data = torch.cat((fake_data, fake_batch.round().to('cpu')), 0)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UQvJHSY0a-s"
      },
      "source": [
        "np.save(os.path.join(opt.expPATH, \"synthetic.npy\"), fake_data.detach().cpu().numpy(), allow_pickle=False)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZOiulo800Ev",
        "outputId": "4af78b1f-0850-4cf8-f60c-c265ddad7e30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        }
      },
      "source": [
        "# gen_samples = fake_data.detach().cpu().numpy()\n",
        "gen_samples = np.load(os.path.join(opt.expPATH, \"synthetic.npy\"), allow_pickle=False)\n",
        "\n",
        "# Load real data\n",
        "real_samples = train_data[0:gen_samples.shape[0], :]\n",
        "\n",
        "# Dimenstion wise probability\n",
        "prob_real = np.mean(real_samples, axis=0)\n",
        "prob_syn = np.mean(gen_samples, axis=0)\n",
        "\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
        "p1 = plt.scatter(prob_real, prob_syn, c=\"b\", alpha=0.7, label=\"CMGAN\", s=9)\n",
        "x_max = max(np.max(prob_real), np.max(prob_syn))\n",
        "x = np.linspace(0, x_max + 0.1, 1000)\n",
        "p2 = plt.plot(x, x, linestyle='-', color='k', label=\"Ideal\")  # solid\n",
        "plt.tick_params(labelsize=12)\n",
        "plt.legend(loc=2, prop={'size': 15})\n",
        "# plt.title('Scatter plot p')\n",
        "# plt.xlabel('x')\n",
        "# plt.ylabel('y')\n",
        "plt.show()\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 960x960 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNBm-qHpSlI-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}